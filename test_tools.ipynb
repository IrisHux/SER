{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032a6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad571a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Path: E:/Unitec/SER/audio\\dataset\\IEMOCAP\n",
      "Evaluation Dataset Path: E:/Unitec/SER/audio\\dataset\\CREMA-D\n",
      "IEMOCAP Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\n",
      "CREMA-D Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\n",
      "IEMOCAP Emotions: ['ang', 'neu', 'sad', 'hap']\n",
      "CREMA-D Emotions: ['ang', 'neu', 'sad', 'hap']\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    print(\"Training Dataset Path:\", CONFIG.dataset_path(\"training\"))\n",
    "    print(\"Evaluation Dataset Path:\", CONFIG.dataset_path(\"evaluation\"))\n",
    "    print(\"IEMOCAP Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()))\n",
    "    print(\"IEMOCAP Emotions:\", CONFIG.dataset_emotions(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Emotions:\", CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4e5e1",
   "metadata": {},
   "source": [
    "## test processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b6db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F000.wav</td>\n",
       "      <td>Excuse me.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F001.wav</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F002.wav</td>\n",
       "      <td>Is there a problem?</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F005.wav</td>\n",
       "      <td>Well what's the problem?  Let me change it.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F012.wav</td>\n",
       "      <td>That's out of control.</td>\n",
       "      <td>ang</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path           audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F000.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F001.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F002.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F005.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F012.wav   \n",
       "\n",
       "                                          text emotion   session  \n",
       "0                                   Excuse me.     neu  Session1  \n",
       "1                                        Yeah.     neu  Session1  \n",
       "2                          Is there a problem?     neu  Session1  \n",
       "3  Well what's the problem?  Let me change it.     neu  Session1  \n",
       "4                       That's out of control.     ang  Session1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4490 entries, 0 to 4489\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4490 non-null   object\n",
      " 1   audio_filename  4490 non-null   object\n",
      " 2   text            4490 non-null   object\n",
      " 3   emotion         4490 non-null   object\n",
      " 4   session         4490 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 175.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "neu    1708\n",
       "ang    1103\n",
       "sad    1084\n",
       "hap     595\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per Session:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "session\n",
       "Session3    1000\n",
       "Session1     942\n",
       "Session5     942\n",
       "Session2     813\n",
       "Session4     793\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from core.config import CONFIG\n",
    "from preprocessing.iemocap import IemocapPreprocessor\n",
    "\n",
    "# 确保 CONFIG 已经加载了配置文件\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# 初始化 IemocapPreprocessor，使用配置中训练数据集的路径\n",
    "# 假设您想验证训练数据的加载\n",
    "iemocap_dataset_path = CONFIG.dataset_path(\"training\")\n",
    "iemocap_preprocessor = IemocapPreprocessor(iemocap_dataset_path)\n",
    "\n",
    "# 生成 DataFrame\n",
    "iemocap_df = iemocap_preprocessor.generate_dataframe()\n",
    "\n",
    "# 显示 DataFrame 的前几行\n",
    "print(\"DataFrame head:\")\n",
    "display(iemocap_df.head())\n",
    "\n",
    "# 您还可以打印一些关于 DataFrame 的信息来进一步验证\n",
    "print(\"\\nDataFrame Info:\")\n",
    "iemocap_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(iemocap_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per Session:\")\n",
    "display(iemocap_df['session'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823ad63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>hap</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>neu</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_SAD_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>sad</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_IEO_ANG_HI.wav</td>\n",
       "      <td>It's eleven o'clock</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path       audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_ANG_XX.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_HAP_XX.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_NEU_XX.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_SAD_XX.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_IEO_ANG_HI.wav   \n",
       "\n",
       "                    text emotion speaker  \n",
       "0  Don't forget a jacket     ang    1001  \n",
       "1  Don't forget a jacket     hap    1001  \n",
       "2  Don't forget a jacket     neu    1001  \n",
       "3  Don't forget a jacket     sad    1001  \n",
       "4    It's eleven o'clock     ang    1001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4900 entries, 0 to 4899\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4900 non-null   object\n",
      " 1   audio_filename  4900 non-null   object\n",
      " 2   text            4900 non-null   object\n",
      " 3   emotion         4900 non-null   object\n",
      " 4   speaker         4900 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 191.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "ang    1271\n",
       "hap    1271\n",
       "sad    1271\n",
       "neu    1087\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "1001    54\n",
       "1047    54\n",
       "1067    54\n",
       "1066    54\n",
       "1065    54\n",
       "        ..\n",
       "1076    53\n",
       "1002    53\n",
       "1009    50\n",
       "1008    50\n",
       "1019    50\n",
       "Name: count, Length: 91, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from preprocessing.cremad import CremaDPreprocessor\n",
    "\n",
    "# 假设您想验证训练数据的加载\n",
    "cremad_dataset_path = CONFIG.dataset_path(\"evaluation\")\n",
    "cremad_preprocessor = CremaDPreprocessor(cremad_dataset_path)\n",
    "\n",
    "# 生成 DataFrame\n",
    "cremad_df = cremad_preprocessor.generate_dataframe()\n",
    "\n",
    "# 显示 DataFrame 的前几行\n",
    "print(\"DataFrame head:\")\n",
    "display(cremad_df.head())\n",
    "\n",
    "# 您还可以打印一些关于 DataFrame 的信息来进一步验证\n",
    "print(\"\\nDataFrame Info:\")\n",
    "cremad_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(cremad_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per speaker:\")\n",
    "display(cremad_df['speaker'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c225ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing models and config ---\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: IEMOCAP\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'iemocap_raw.pkl' ---\n",
      "[INFO] Using IemocapPreprocessor for dataset: IEMOCAP\n",
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\\iemocap_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for IEMOCAP.\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: CREMA-D\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'crema-d_raw.pkl' ---\n",
      "[INFO] Using CremaDPreprocessor for dataset: CREMA-D\n",
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\\crema-d_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for CREMA-D.\n",
      "\n",
      "====================\n",
      "--- All processing complete! ---\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from audio.extractor import WavLMEmotionExtractor\n",
    "from core.config import CONFIG\n",
    "from scripts.preprocess_data import process_raw_data_to_pickle\n",
    "\n",
    "\n",
    "def run_preprocessing_pipeline(dataset_name: str):\n",
    "    \"\"\"\n",
    "    为一个指定的数据集完整地执行数据预处理的三个步骤。\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): 数据集的名称 (从 CONFIG 中获取)。\n",
    "        audio_extractor: 初始化后的音频特征提取器。\n",
    "        text_tokenizer: 初始化后的文本分词器。\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20}\\n[START] Processing dataset: {dataset_name}\\n{'='*20}\")\n",
    "\n",
    "    # 1. 动态生成文件名，避免硬编码\n",
    "    # 例如从 \"IEMOCAP_full_release\" 生成 \"iemocap\" 作为文件名前缀\n",
    "    base_name = dataset_name.split('_')[0].lower() \n",
    "    raw_file = f\"{base_name}_raw.pkl\"\n",
    "    audio_file = f\"{base_name}_audio_features.pkl\"\n",
    "    text_file = f\"{base_name}_text_tokens.pkl\"\n",
    "\n",
    "    # 2. 按顺序执行数据处理流程\n",
    "    print(f\"--- Step 1: Processing raw data to '{raw_file}' ---\")\n",
    "    process_raw_data_to_pickle(dataset_name, raw_file)\n",
    "\n",
    "\n",
    "    print(f\"\\n[SUCCESS] Finished processing for {dataset_name}.\")\n",
    "\n",
    "\n",
    "# --- 主执行脚本 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 集中进行初始化\n",
    "    print(\"--- Initializing models and config ---\")\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    # 2. 定义需要处理的数据集列表\n",
    "    datasets_to_process = [\n",
    "        CONFIG.training_dataset_name(),\n",
    "        CONFIG.evaluation_dataset_name()\n",
    "    ]\n",
    "\n",
    "    # 3. 循环调用处理流程\n",
    "    for name in datasets_to_process:\n",
    "        run_preprocessing_pipeline(name)\n",
    "\n",
    "    print(f\"\\n{'='*20}\\n--- All processing complete! ---\\n{'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5c8c4",
   "metadata": {},
   "source": [
    "## 实例化数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b09ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 IEMOCAP 数据集用于训练...\n",
      "--- 正在为数据集 'IEMOCAP' 准备Dataloaders ---\n",
      "[INFO] 已从以下路径加载音频特征: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_audio_features.pkl\n",
      "[INFO] 已从以下路径加载文本Tokens: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_text_tokens.pkl\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- 训练和验证流程 ---\n",
    "print(\"加载 IEMOCAP 数据集用于训练...\")\n",
    "# 只需一行代码，即可获取训练和验证所需的所有 dataloader\n",
    "iemocap_loaders = get_dataloaders(CONFIG.training_dataset_name())\n",
    "train_loader = iemocap_loaders['train']\n",
    "validation_loader = iemocap_loaders['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd0b9d",
   "metadata": {},
   "source": [
    "### train baseline model（New）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a66ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在加载 'IEMOCAP' 数据集用于训练和验证 ---\n",
      "--- 正在为数据集 'IEMOCAP' 准备Dataloaders ---\n",
      "[INFO] 使用内存优化模式\n",
      "[INFO] 已从以下路径加载音频特征: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_audio_features.pkl\n",
      "[INFO] 已从以下路径加载文本Tokens: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_text_tokens.pkl\n",
      "[INFO] 使用内存优化的数据整理器\n",
      "[INFO] 已清理加载过程中的临时内存\n",
      "\n",
      "--- 正在加载 'CREMA-D' 数据集用于零样本评估 ---\n",
      "--- 正在为数据集 'CREMA-D' 准备Dataloaders ---\n",
      "[INFO] 使用内存优化模式\n",
      "[INFO] 已从以下路径加载音频特征: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_audio_features.pkl\n",
      "[INFO] 已从以下路径加载文本Tokens: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_text_tokens.pkl\n",
      "[INFO] 使用内存优化的数据整理器\n",
      "[INFO] 已清理加载过程中的临时内存\n",
      "\n",
      "--- 初始化基线模型和训练器 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['projector.bias', 'projector.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10\\cuda\\CUDACachingAllocator.cpp:803.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\wavlm\\modeling_wavlm.py:1434: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 特征提取层已冻结，使用梯度累积步数: 8\n",
      "\n",
      "--- 开始在 IEMOCAP 上训练基线模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Rerun the training cell\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import MemoryOptimizedAudioBaselineTrainer\n",
    "\n",
    "# 设置CUDA内存优化\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- 训练和验证流程 ---\n",
    "training_dataset_name = CONFIG.training_dataset_name()\n",
    "print(f\"\\n--- 正在加载 '{training_dataset_name}' 数据集用于训练和验证 ---\")\n",
    "\n",
    "# 使用内存优化版本的dataloaders（这是主要的改动）\n",
    "try:\n",
    "    # 使用内存优化版本\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name, use_memory_optimization=True)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "    \n",
    "    # --- 零样本评估流程 (在 CREMA-D 上) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{evaluation_dataset_name}' 数据集用于零样本评估 ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name, use_memory_optimization=True)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    # --- 实例化模型和训练器 ---\n",
    "    print(\"\\n--- 初始化基线模型和训练器 ---\")\n",
    "    \n",
    "    # 清理内存后创建模型\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 获取情感标签\n",
    "    iemocap_emotions = CONFIG.dataset_emotions(training_dataset_name)\n",
    "    num_labels = len(iemocap_emotions)\n",
    "    \n",
    "    # 创建模型（考虑使用更小的batch_size)\n",
    "    baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "    \n",
    "    # 使用内存优化版本的训练器\n",
    "    baseline_trainer = MemoryOptimizedAudioBaselineTrainer(  # 使用新的训练器\n",
    "        model=baseline_model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate() * 4,  # 由于梯度累积，需要调整学习率\n",
    "        optimizer_type=CONFIG.optimizer_type(),\n",
    "        gradient_accumulation_steps=8  # 梯度累积步数，可以根据需要调整\n",
    "    )\n",
    "    \n",
    "    # --- 步骤 3: 训练模型 ---\n",
    "    print(\"\\n--- 开始在 IEMOCAP 上训练基线模型 ---\")\n",
    "    baseline_trainer.train(train_loader)\n",
    "    \n",
    "    # --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\n",
    "    print(\"\\n--- 在 IEMOCAP 验证集上评估模型性能 ---\")\n",
    "    baseline_trainer.eval(validation_loader, labels=iemocap_emotions)\n",
    "    \n",
    "    # --- 步骤 5: 在 CREMA-D 测试集上进行零样本评估 ---\n",
    "    print(\"\\n--- 在 CREMA-D 测试集上进行零样本评估 ---\")\n",
    "    cremad_emotions = CONFIG.dataset_emotions(evaluation_dataset_name)\n",
    "    baseline_trainer.eval(evaluation_loader, labels=cremad_emotions)\n",
    "    \n",
    "    print(\"\\n--- 基线模型训练和评估完成！ ---\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\n[ERROR] CUDA内存不足: {e}\")\n",
    "    print(\"建议:\")\n",
    "    print(\"1. 进一步减小batch_size到1\")\n",
    "    print(\"2. 减少音频最大长度\")\n",
    "    print(\"3. 使用更小的模型variant\")\n",
    "    print(\"4. 重启运行时清理内存\")\n",
    "    \n",
    "    # 清理内存\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] 训练过程中出现错误: {e}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697ad0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Check PyTorch and CUDA ---\n",
      "Python Version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "PyTorch Version: 2.1.2+cu121\n",
      "Is CUDA available: True\n",
      "\n",
      "--- 2. Get GPU Device Information ---\n",
      "Default CUDA device: cuda:0\n",
      "GPU Name: NVIDIA L40S-6Q\n",
      "PyTorch compiled with CUDA version: 12.1\n",
      "\n",
      "--- 3. Test Data Transfer Between CPU and GPU ---\n",
      "a. Tensor created on the CPU: tensor([1, 2, 3])\n",
      "   - Device: cpu\n",
      "\n",
      "[ERROR] Failed to move data to GPU: CUDA error: operation not supported\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"--- 1. Check PyTorch and CUDA ---\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available: {is_cuda_available}\")\n",
    "\n",
    "if not is_cuda_available:\n",
    "    print(\"\\n[ERROR] PyTorch could not detect CUDA. Please check your NVIDIA driver and PyTorch installation.\")\n",
    "    # If CUDA is not available, exit the script\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 2. Get GPU Device Information ---\")\n",
    "# Get the default CUDA device (usually GPU 0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Default CUDA device: {device}\")\n",
    "\n",
    "# Print the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "# Print the CUDA version PyTorch was compiled with\n",
    "torch_cuda_version = torch.version.cuda\n",
    "print(f\"PyTorch compiled with CUDA version: {torch_cuda_version}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. Test Data Transfer Between CPU and GPU ---\")\n",
    "# a. Create a tensor on the CPU\n",
    "cpu_tensor = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(f\"a. Tensor created on the CPU: {cpu_tensor}\")\n",
    "print(f\"   - Device: {cpu_tensor.device}\")\n",
    "\n",
    "# b. Try to move the tensor to the GPU\n",
    "try:\n",
    "    gpu_tensor = cpu_tensor.to(device)\n",
    "    print(f\"\\nb. Successfully moved tensor to GPU: {gpu_tensor}\")\n",
    "    print(f\"   - Device: {gpu_tensor.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move data to GPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. Test Computation on GPU ---\")\n",
    "# a. Create two tensors on the GPU for computation\n",
    "try:\n",
    "    a = torch.randn(3, 3).to(device)\n",
    "    b = torch.randn(3, 3).to(device)\n",
    "    print(f\"a. Created two 3x3 random tensors on the GPU.\")\n",
    "    print(f\"   - Tensor a device: {a.device}\")\n",
    "    print(f\"   - Tensor b device: {b.device}\")\n",
    "\n",
    "    # b. Perform matrix multiplication on the GPU\n",
    "    print(\"\\nb. Performing matrix multiplication on GPU (c = a * b)...\")\n",
    "    c = torch.matmul(a, b)\n",
    "    print(f\"   - Result c device: {c.device}\")\n",
    "    print(f\"   - Computation successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Computation on GPU failed: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 5. Test Moving Result Back to CPU ---\")\n",
    "# a. Move the computation result from GPU back to CPU\n",
    "try:\n",
    "    result_cpu_tensor = c.cpu()\n",
    "    print(\"a. Successfully moved the computation result back to the CPU.\")\n",
    "    print(f\"   - Device: {result_cpu_tensor.device}\")\n",
    "    print(\"\\nComputation result:\")\n",
    "    print(result_cpu_tensor)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move result back to CPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- All tests completed ---\")\n",
    "print(\"[SUCCESS] Your PyTorch and CUDA environment is configured correctly, and they can communicate and perform computations normally!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9cce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 增强型 PyTorch & NVIDIA vGPU 环境诊断工具 ---\n",
      "诊断开始时间: 2025-08-19 17:11:45\n",
      "操作系统: Windows 10\n",
      "\n",
      "--- 1. 系统和驱动信息 ---\n",
      "🐍 Python 版本: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "🔥 PyTorch 版本: 2.1.2+cu121\n",
      "\n",
      "... 正在运行 `nvidia-smi` 获取驱动信息...\n",
      "✅ `nvidia-smi` 命令成功执行。\n",
      "   - GPU 型号: NVIDIA L40S-6Q               WDDM\n",
      "   - 驱动版本: 538.95\n",
      "   - 驱动支持的最高 CUDA 版本: 12.2\n",
      "\n",
      "   - [关键信息] 检测到 vGPU 环境！将执行 vGPU 许可证诊断。\n",
      "\n",
      "--- 2. vGPU 许可证服务诊断 ---\n",
      "   - 正在检查 Windows 服务: 'NVIDIA Display Container LS'...\n",
      "   - 服务状态: 未找到或未安装 (Not Found/Installed)\n",
      "   - [严重] NVIDIA 许可证服务未运行！这是导致功能受限的直接原因。\n",
      "   - 解决方案: 请启动该服务。Windows: `net start nvdisplay.container.service`, Linux: `sudo systemctl start nvidia-gridd`\n",
      "📄 正在检查 vGPU 日志文件: C:\\Users\\Public\\Documents\\NvidiaLogging\\Log.NVDisplay.Container.exe.log\n",
      "   - [严重] 在日志中发现可能的许可证错误: 获取许可证失败\n",
      "   - 诊断: GPU 可能因无法获取有效许可证而被限制了计算功能。请检查您的许可证服务器地址、网络连接和客户端配置。\n",
      "\n",
      "--- 3. PyTorch CUDA 验证 ---\n",
      "🔍 PyTorch 是否能找到 CUDA: True\n",
      "✅ PyTorch 可以访问 CUDA。\n",
      "   - PyTorch 编译所用 CUDA 版本: 12.1\n",
      "   - 可用 GPU 数量: 1\n",
      "\n",
      "--- 4. CUDA 核心操作测试 ---\n",
      "a. 尝试在 GPU (cuda:0) 上创建张量...\n",
      "\n",
      "[CRITICAL ERROR] 在执行 CUDA 操作时发生错误: CUDA error: operation not supported\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "[vGPU 诊断]\n",
      "在 vGPU 环境下，此错误（特别是 'operation not supported' 或 'initialization error'）极有可能是由 **许可证问题** 造成的。\n",
      "即使服务正在运行，许可证服务器也可能无法访问或没有可用的许可证。\n",
      "请重点检查第 2 部分的日志分析结果，并联系您的系统管理员确认许可证配置。\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import platform\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def run_command(command, shell=False):\n",
    "    \"\"\"执行一个 shell 命令并返回其输出。\"\"\"\n",
    "    try:\n",
    "        # 在 Windows 上隐藏命令行窗口\n",
    "        startupinfo = None\n",
    "        if platform.system() == \"Windows\":\n",
    "            startupinfo = subprocess.STARTUPINFO()\n",
    "            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n",
    "\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            errors='ignore',\n",
    "            check=True,\n",
    "            shell=shell,\n",
    "            startupinfo=startupinfo\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except (FileNotFoundError, subprocess.CalledProcessError) as e:\n",
    "        return f\"命令执行失败: {e}\"\n",
    "\n",
    "def check_windows_service(service_name):\n",
    "    \"\"\"在 Windows 上检查指定服务的状态。\"\"\"\n",
    "    output = run_command(['sc', 'query', service_name])\n",
    "    if \"STATE\" in output and \"RUNNING\" in output:\n",
    "        return \"正在运行 (Running)\"\n",
    "    elif \"FAILED\" in output or \"1060\" in output: # 1060: The specified service does not exist\n",
    "        return \"未找到或未安装 (Not Found/Installed)\"\n",
    "    else:\n",
    "        return \"已停止 (Stopped)\"\n",
    "\n",
    "def check_linux_service(service_name):\n",
    "    \"\"\"在 Linux 上检查指定服务的状态。\"\"\"\n",
    "    output = run_command(f\"systemctl is-active {service_name}\")\n",
    "    if \"active\" in output:\n",
    "        return \"正在运行 (Active)\"\n",
    "    elif \"inactive\" in output:\n",
    "        return \"已停止 (Inactive)\"\n",
    "    else:\n",
    "        return \"未找到或状态未知 (Not Found or Unknown)\"\n",
    "\n",
    "def analyze_vgpu_logs():\n",
    "    \"\"\"查找并分析 NVIDIA vGPU 许可证日志文件。\"\"\"\n",
    "    log_path = \"\"\n",
    "    system = platform.system()\n",
    "    if system == \"Windows\":\n",
    "        log_path = \"C:\\\\Users\\\\Public\\\\Documents\\\\NvidiaLogging\\\\Log.NVDisplay.Container.exe.log\"\n",
    "    elif system == \"Linux\":\n",
    "        log_path = \"/var/log/nvidia/gridd.log\"\n",
    "\n",
    "    print(f\"📄 正在检查 vGPU 日志文件: {log_path}\")\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        print(\"   - [警告] 未找到日志文件。可能服务从未运行过，或日志在其他位置。\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            # 只读取最后 100 行以提高效率\n",
    "            log_content = f.readlines()[-100:]\n",
    "            log_content = \"\".join(log_content)\n",
    "\n",
    "        # 常见的许可证错误关键词\n",
    "        error_keywords = {\n",
    "            \"Failed to acquire license\": \"获取许可证失败\",\n",
    "            \"Failed to connect to license server\": \"连接到许可证服务器失败\",\n",
    "            \"could not connect to\": \"无法连接到\",\n",
    "            \"Connection refused\": \"连接被拒绝\",\n",
    "            \"License request failed\": \"许可证请求失败\",\n",
    "            \"unlicensed\": \"未授权状态\",\n",
    "            \"terminated\": \"已终止\"\n",
    "        }\n",
    "\n",
    "        found_errors = []\n",
    "        for key, value in error_keywords.items():\n",
    "            if re.search(key, log_content, re.IGNORECASE):\n",
    "                found_errors.append(value)\n",
    "        \n",
    "        if found_errors:\n",
    "            print(f\"   - [严重] 在日志中发现可能的许可证错误: {', '.join(found_errors)}\")\n",
    "            print(\"   - 诊断: GPU 可能因无法获取有效许可证而被限制了计算功能。请检查您的许可证服务器地址、网络连接和客户端配置。\")\n",
    "        else:\n",
    "            print(\"   - [信息] 在最近的日志中未发现明显的许可证错误。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   - [错误] 读取日志文件失败: {e}\")\n",
    "\n",
    "# --- 主诊断流程 ---\n",
    "print(\"--- 增强型 PyTorch & NVIDIA vGPU 环境诊断工具 ---\")\n",
    "print(f\"诊断开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"操作系统: {platform.system()} {platform.release()}\")\n",
    "\n",
    "print(\"\\n--- 1. 系统和驱动信息 ---\")\n",
    "print(f\"🐍 Python 版本: {sys.version.splitlines()[0]}\")\n",
    "print(f\"🔥 PyTorch 版本: {torch.__version__}\")\n",
    "\n",
    "print(\"\\n... 正在运行 `nvidia-smi` 获取驱动信息...\")\n",
    "nvidia_smi_output = run_command(['nvidia-smi'])\n",
    "if \"命令执行失败\" not in nvidia_smi_output:\n",
    "    print(\"✅ `nvidia-smi` 命令成功执行。\")\n",
    "    driver_match = re.search(r\"Driver Version: ([\\d\\.]+)\", nvidia_smi_output)\n",
    "    cuda_match = re.search(r\"CUDA Version: ([\\d\\.]+)\", nvidia_smi_output)\n",
    "    gpu_name_match = re.search(r\"\\d+\\s+(NVIDIA\\s[\\w\\s-]+)\\s+\", nvidia_smi_output)\n",
    "    \n",
    "    driver_version = driver_match.group(1) if driver_match else \"未检测到\"\n",
    "    driver_cuda_version = cuda_match.group(1) if cuda_match else \"未检测到\"\n",
    "    gpu_name = gpu_name_match.group(1).strip() if gpu_name_match else \"未检测到\"\n",
    "    \n",
    "    print(f\"   - GPU 型号: {gpu_name}\")\n",
    "    print(f\"   - 驱动版本: {driver_version}\")\n",
    "    print(f\"   - 驱动支持的最高 CUDA 版本: {driver_cuda_version}\")\n",
    "\n",
    "    # 检查是否为 vGPU 环境\n",
    "    if \"vGPU\" in nvidia_smi_output or re.search(r\"\\w+-\\d+Q\", gpu_name):\n",
    "        print(\"\\n   - [关键信息] 检测到 vGPU 环境！将执行 vGPU 许可证诊断。\")\n",
    "        is_vgpu = True\n",
    "    else:\n",
    "        is_vgpu = False\n",
    "else:\n",
    "    print(\"\\n[CRITICAL ERROR] `nvidia-smi` 命令执行失败。\")\n",
    "    print(\"无法验证 NVIDIA 驱动安装。请确保驱动已正确安装且 `nvidia-smi` 在系统 PATH 中。\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- vGPU 许可证诊断部分 ---\n",
    "if is_vgpu:\n",
    "    print(\"\\n--- 2. vGPU 许可证服务诊断 ---\")\n",
    "    system = platform.system()\n",
    "    service_status = \"\"\n",
    "    if system == \"Windows\":\n",
    "        service_name = \"NVIDIA Display Container LS\"\n",
    "        print(f\"   - 正在检查 Windows 服务: '{service_name}'...\")\n",
    "        service_status = check_windows_service(\"nvdisplay.container.service\")\n",
    "        print(f\"   - 服务状态: {service_status}\")\n",
    "    elif system == \"Linux\":\n",
    "        service_name = \"nvidia-gridd\"\n",
    "        print(f\"   - 正在检查 Linux 服务: '{service_name}'...\")\n",
    "        service_status = check_linux_service(service_name)\n",
    "        print(f\"   - 服务状态: {service_status}\")\n",
    "    \n",
    "    if \"正在运行\" not in service_status and \"Active\" not in service_status:\n",
    "        print(\"   - [严重] NVIDIA 许可证服务未运行！这是导致功能受限的直接原因。\")\n",
    "        print(\"   - 解决方案: 请启动该服务。Windows: `net start nvdisplay.container.service`, Linux: `sudo systemctl start nvidia-gridd`\")\n",
    "    else:\n",
    "        print(\"   - ✅ 服务正在运行。\")\n",
    "\n",
    "    analyze_vgpu_logs()\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. PyTorch CUDA 验证 ---\")\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"🔍 PyTorch 是否能找到 CUDA: {is_cuda_available}\")\n",
    "\n",
    "if not is_cuda_available:\n",
    "    print(\"\\n[CRITICAL ERROR] PyTorch 报告 CUDA 不可用。\")\n",
    "    print(\"常见原因:\")\n",
    "    print(\"  1. 您可能安装了仅 CPU 版本的 PyTorch。请访问官网 (pytorch.org) 获取正确的 CUDA 版本安装命令。\")\n",
    "    print(\"  2. NVIDIA 驱动与 PyTorch 的 CUDA 工具包版本不兼容。\")\n",
    "    if is_vgpu:\n",
    "        print(\"  3. (vGPU 环境) 许可证获取失败导致 GPU 计算功能被禁用，PyTorch 无法访问。\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"✅ PyTorch 可以访问 CUDA。\")\n",
    "print(f\"   - PyTorch 编译所用 CUDA 版本: {torch.version.cuda}\")\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"   - 可用 GPU 数量: {device_count}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. CUDA 核心操作测试 ---\")\n",
    "if device_count > 0:\n",
    "    try:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"a. 尝试在 GPU (cuda:0) 上创建张量...\")\n",
    "        a = torch.randn(3, 3, device=device)\n",
    "        print(f\"   - ✅ 成功在 {a.device} ({gpu_name}) 上创建张量。\")\n",
    "\n",
    "        print(f\"b. 尝试在 GPU 上执行矩阵乘法...\")\n",
    "        b = torch.randn(3, 3, device=device)\n",
    "        c = torch.matmul(a, b)\n",
    "        print(f\"   - ✅ 计算成功，结果位于 {c.device}。\")\n",
    "        \n",
    "        print(f\"c. 尝试将结果移回 CPU...\")\n",
    "        result_cpu = c.cpu()\n",
    "        print(f\"   - ✅ 成功将结果移回 {result_cpu.device}。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[CRITICAL ERROR] 在执行 CUDA 操作时发生错误: {e}\")\n",
    "        if is_vgpu:\n",
    "            print(\"\\n[vGPU 诊断]\")\n",
    "            print(\"在 vGPU 环境下，此错误（特别是 'operation not supported' 或 'initialization error'）极有可能是由 **许可证问题** 造成的。\")\n",
    "            print(\"即使服务正在运行，许可证服务器也可能无法访问或没有可用的许可证。\")\n",
    "            print(\"请重点检查第 2 部分的日志分析结果，并联系您的系统管理员确认许可证配置。\")\n",
    "        else:\n",
    "            print(\"\\n[通用诊断]\")\n",
    "            print(\"此错误可能由驱动不稳定、硬件问题或 PyTorch 与驱动的深层不兼容导致。建议首先尝试重启系统和更新驱动程序。\")\n",
    "        sys.exit()\n",
    "else:\n",
    "    print(\"[警告] 没有可用的 GPU 设备进行操作测试。\")\n",
    "\n",
    "\n",
    "print(\"\\n-------------------------------------------------\")\n",
    "print(\"✅ [诊断完成] 核心 CUDA 操作测试通过！\")\n",
    "if is_vgpu:\n",
    "    print(\"您的 vGPU 环境似乎已为 PyTorch 准备就绪。如果遇到问题，请首先关注许可证服务的状态和日志。\")\n",
    "else:\n",
    "    print(\"您的 PyTorch 环境已正确配置，可以使用 NVIDIA GPU。\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce7a1f",
   "metadata": {},
   "source": [
    "### （pass）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa1a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mget_dataloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataloaders\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineTrainer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    主函数，用于执行声学基线模型的完整训练和评估流程。\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\audio\\trainer.py:16\u001b[39m\n\u001b[32m     13\u001b[39m torch.set_float32_matmul_precision(\u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 可选：首次预热，避免第一次调用卡顿\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m _ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# class AudioTrainer(AbstractTrainer):\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#     def __init__(\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#         self,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 假设 AbstractTrainer 已经从 core/trainer.py 导入或在notebook中定义\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，用于执行声学基线模型的完整训练和评估流程。\n",
    "    \"\"\"\n",
    "    # 1. 加载配置文件\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "    print(f\"--- 实验配置已加载 ---\")\n",
    "    print(f\"使用的设备: {device}\")\n",
    "\n",
    "    # --- 训练和验证流程 ---\n",
    "    training_dataset_name = CONFIG.training_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{training_dataset_name}' 数据集用于训练和验证 ---\")\n",
    "    \n",
    "    # 使用高级函数获取训练和验证所需的所有 dataloader\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "\n",
    "    # 3. 初始化模型\n",
    "    # 从配置中获取情感标签列表，以确定模型的输出维度\n",
    "    num_labels = len(CONFIG.dataset_emotions(training_dataset_name))\n",
    "    print(f\"\\n--- 正在初始化 AudioBaselineModel (类别数: {num_labels}) ---\")\n",
    "    model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "    # 4. 初始化训练器\n",
    "    print(f\"--- 正在初始化 AudioBaselineTrainer ---\")\n",
    "    trainer = AudioBaselineTrainer(\n",
    "        model=model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate(),\n",
    "        optimizer_type=CONFIG.optimizer_type()\n",
    "    )\n",
    "\n",
    "    # 5. 开始训练\n",
    "    trainer.train(train_loader)\n",
    "\n",
    "    # 6. 在验证集上评估\n",
    "    print(f\"\\n--- 正在 '{training_dataset_name}' 的验证集上进行评估 ---\")\n",
    "    trainer.eval(validation_loader, labels=CONFIG.dataset_emotions(training_dataset_name))\n",
    "\n",
    "    # --- 零样本评估流程 (在 CREMA-D 上) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{evaluation_dataset_name}' 数据集用于零样本评估 ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    print(f\"\\n--- 正在 '{evaluation_dataset_name}' 上进行零样本评估 ---\")\n",
    "    trainer.eval(evaluation_loader, labels=CONFIG.dataset_emotions(evaluation_dataset_name))\n",
    "    \n",
    "    print(\"\\n--- 训练和评估流程全部完成 ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d06293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "已创建训练集，包含 3592 个样本。\n",
      "已创建验证集，包含 898 个样本。\n",
      "已创建训练集，包含 3920 个样本。\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 第 1 步: 加载必要的库和你的自定义类 ---\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, DebertaV2Tokenizer\n",
    "import os\n",
    "\n",
    "from dataloaders.dataset import CustomSERDataset\n",
    "from scripts.get_dataloaders import CustomDataCollator\n",
    "\n",
    "\n",
    "# --- 确定计算设备 ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 第 2 步: 初始化处理器和分词器 ---\n",
    "# 这些是collator需要的 \"工具\"\n",
    "audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG.audio_encoder_name())\n",
    "text_tokenizer = DebertaV2Tokenizer.from_pretrained(CONFIG.text_encoder_name())\n",
    "\n",
    "# --- 第 3 步: 实例化训练数据集 (IEMOCAP) ---\n",
    "# 注意：你需要先运行你的预处理脚本，生成统一的元数据文件\n",
    "\n",
    "\n",
    "# 定义最大长度 (例如10秒)\n",
    "MAX_LEN_IN_SECONDS = 10\n",
    "max_audio_len = 16000 * MAX_LEN_IN_SECONDS\n",
    "\n",
    "iemocap_emotions = CONFIG.dataset_emotions(CONFIG.training_dataset_name())\n",
    "train_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename), # 使用配置和生成的文件名\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='train',\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "# 创建IEMOCAP验证数据集\n",
    "val_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename),\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='val',\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "# --- 第 4 步: 实例化评估数据集 (CREMA-D) ---\n",
    "cremad_emotions = CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name())\n",
    "eval_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()),cremad_metadata_filename), # 使用配置和生成的文件名\n",
    "    emotions=cremad_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "\n",
    "# --- 第 5 步: 实例化你的数据整理器 ---\n",
    "# 数据整理器对于训练集和评估集是通用的\n",
    "data_collator = CustomDataCollator(\n",
    "    audio_processor=audio_processor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    # device=device # device 来自你的CONFIG或Notebook顶部定义\n",
    ")\n",
    "\n",
    "# --- 第 6 步: 创建训练集的 DataLoader ---\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=True, # 训练集通常需要打乱\n",
    "    collate_fn=data_collator, # 关键！在这里传入你的自定义整理器\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator, \n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'], \n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "# --- 验证batch size ---\n",
    "print(train_dataloader.batch_size)\n",
    "\n",
    "# --- 第 7 步: 创建评估集的 DataLoader ---\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=False, # 评估集通常不需要打乱\n",
    "    collate_fn=data_collator, # 关键！在这里传入你的自定义整理器\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf763c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 验证 DataLoader 和 Collator (详细调试模式) ---\n",
      "\n",
      "验证训练集 DataLoader:\n",
      "成功从 DataLoader 获取 CPU 批次。\n",
      "\n",
      "开始逐个将张量移动到 GPU...\n",
      "准备移动 'audio_input_values'... | 类型: torch.float32 | 形状: torch.Size([1, 32, 160000])\n",
      "'audio_input_values' 移动成功！\n",
      "准备移动 'text_input_ids'... | 类型: torch.int64 | 形状: torch.Size([32, 54])\n",
      "'text_input_ids' 移动成功！\n",
      "准备移动 'text_attention_mask'... | 类型: torch.int64 | 形状: torch.Size([32, 54])\n",
      "'text_attention_mask' 移动成功！\n",
      "准备移动 'labels'... | 类型: torch.int64 | 形状: torch.Size([32])\n",
      "'labels' 移动成功！\n",
      "\n",
      "所有张量均已成功移动到 GPU！\n",
      "--- 验证 DataLoader 和 Collator ---\n",
      "\n",
      "验证训练集 DataLoader:\n",
      "成功从训练集 DataLoader 获取一个批次！\n",
      "批次包含的键: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "音频输入形状: torch.Size([1, 32, 160000])\n",
      "文本输入形状: torch.Size([32, 53])\n",
      "标签形状: torch.Size([32])\n",
      "\n",
      "验证评估集 DataLoader:\n",
      "成功从评估集 DataLoader 获取一个批次！\n",
      "批次包含的键: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "音频输入形状: torch.Size([1, 32, 160000])\n",
      "文本输入形状: torch.Size([32, 11])\n",
      "标签形状: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# --- 第 8 步 (验证): 精确定位问题的详细调试 ---\n",
    "print(\"--- 验证 DataLoader 和 Collator (详细调试模式) ---\")\n",
    "print(\"\\n验证训练集 DataLoader:\")\n",
    "\n",
    "# 初始化一个空的 gpu_batch 字典\n",
    "gpu_batch = {}\n",
    "try:\n",
    "    # 1. 获取 CPU 批次 (这一步已经成功)\n",
    "    cpu_batch = next(iter(train_dataloader))\n",
    "    print(\"成功从 DataLoader 获取 CPU 批次。\")\n",
    "\n",
    "    # 2. 逐个检查并移动张量\n",
    "    print(\"\\n开始逐个将张量移动到 GPU...\")\n",
    "\n",
    "    # 检查 'audio_input_values'\n",
    "    key = 'audio_input_values'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'text_input_ids'\n",
    "    key = 'text_input_ids'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'text_attention_mask'\n",
    "    key = 'text_attention_mask'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'labels'\n",
    "    key = 'labels'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    print(\"\\n所有张量均已成功移动到 GPU！\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 如果出错，我们会明确知道是在处理哪个 key 时发生的\n",
    "    print(f\"\\n在尝试移动 '{key}' 张量时出错: {e}\")\n",
    "\n",
    "# --- 第 8 步 (验证): 从DataLoader中取出一个批次，检查其内容 ---\n",
    "print(\"--- 验证 DataLoader 和 Collator ---\")\n",
    "print(\"\\n验证训练集 DataLoader:\")\n",
    "try:\n",
    "    first_train_batch = next(iter(train_dataloader))\n",
    "    print(\"成功从训练集 DataLoader 获取一个批次！\")\n",
    "    print(\"批次包含的键:\", first_train_batch.keys())\n",
    "    print(\"音频输入形状:\", first_train_batch['audio_input_values'].shape)\n",
    "    print(\"文本输入形状:\", first_train_batch['text_input_ids'].shape)\n",
    "    print(\"标签形状:\", first_train_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"获取训练集批次时出错: {e}\")\n",
    "\n",
    "print(\"\\n验证评估集 DataLoader:\")\n",
    "try:\n",
    "    first_eval_batch = next(iter(eval_dataloader))\n",
    "    print(\"成功从评估集 DataLoader 获取一个批次！\")\n",
    "    print(\"批次包含的键:\", first_eval_batch.keys())\n",
    "    print(\"音频输入形状:\", first_eval_batch['audio_input_values'].shape)\n",
    "    print(\"文本输入形状:\", first_eval_batch['text_input_ids'].shape)\n",
    "    print(\"标签形状:\", first_eval_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"获取评估集批次时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde94d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 初始化基线模型和训练器 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 正在冻结WavLM的特征提取层...\n",
      "[INFO] 特征提取层已冻结。\n",
      "\n",
      "--- 开始在 IEMOCAP 上训练基线模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▎      | 38/113 [2:23:33<4:43:21, 226.68s/it, accuracy=0.406, loss=1.3]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --- 步骤 3: 训练模型 ---\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 开始在 IEMOCAP 上训练基线模型 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mbaseline_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 在 IEMOCAP 验证集上评估模型性能 ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\core\\trainer.py:57\u001b[39m, in \u001b[36mAbstractTrainer.train\u001b[39m\u001b[34m(self, train_dataloader)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "# 实例化模型和训练器 ---\n",
    "print(\"\\n--- 初始化基线模型和训练器 ---\")\n",
    "num_labels = len(iemocap_emotions)\n",
    "baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "baseline_trainer = AudioBaselineTrainer(\n",
    "    model=baseline_model,\n",
    "    num_epochs=CONFIG.training_epochs(),\n",
    "    learning_rate=CONFIG.learning_rate(),\n",
    "    optimizer_type=CONFIG.optimizer_type()\n",
    ")\n",
    "\n",
    "# --- 步骤 3: 训练模型 ---\n",
    "print(\"\\n--- 开始在 IEMOCAP 上训练基线模型 ---\")\n",
    "baseline_trainer.train(train_dataloader)\n",
    "\n",
    "# --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\n",
    "print(\"\\n--- 在 IEMOCAP 验证集上评估模型性能 ---\")\n",
    "baseline_trainer.eval(val_dataloader, labels=iemocap_emotions)\n",
    "\n",
    "# --- 步骤 5: 在 CREMA-D 测试集上进行零样本评估 ---\n",
    "print(\"\\n--- 在 CREMA-D 测试集上进行零样本评估 ---\")\n",
    "baseline_trainer.eval(eval_dataloader, labels=cremad_emotions)\n",
    "\n",
    "print(\"\\n--- 基线模型训练和评估完成！ ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56fd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # 训练前先清理一次显存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
