{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032a6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad571a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Path: E:/Unitec/SER/audio\\dataset\\IEMOCAP\n",
      "Evaluation Dataset Path: E:/Unitec/SER/audio\\dataset\\CREMA-D\n",
      "IEMOCAP Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\n",
      "CREMA-D Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\n",
      "IEMOCAP Emotions: ['ang', 'neu', 'sad', 'hap']\n",
      "CREMA-D Emotions: ['ang', 'neu', 'sad', 'hap']\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    print(\"Training Dataset Path:\", CONFIG.dataset_path(\"training\"))\n",
    "    print(\"Evaluation Dataset Path:\", CONFIG.dataset_path(\"evaluation\"))\n",
    "    print(\"IEMOCAP Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()))\n",
    "    print(\"IEMOCAP Emotions:\", CONFIG.dataset_emotions(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Emotions:\", CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4e5e1",
   "metadata": {},
   "source": [
    "## test processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b6db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F000.wav</td>\n",
       "      <td>Excuse me.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F001.wav</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F002.wav</td>\n",
       "      <td>Is there a problem?</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F005.wav</td>\n",
       "      <td>Well what's the problem?  Let me change it.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F012.wav</td>\n",
       "      <td>That's out of control.</td>\n",
       "      <td>ang</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path           audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F000.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F001.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F002.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F005.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F012.wav   \n",
       "\n",
       "                                          text emotion   session  \n",
       "0                                   Excuse me.     neu  Session1  \n",
       "1                                        Yeah.     neu  Session1  \n",
       "2                          Is there a problem?     neu  Session1  \n",
       "3  Well what's the problem?  Let me change it.     neu  Session1  \n",
       "4                       That's out of control.     ang  Session1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4490 entries, 0 to 4489\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4490 non-null   object\n",
      " 1   audio_filename  4490 non-null   object\n",
      " 2   text            4490 non-null   object\n",
      " 3   emotion         4490 non-null   object\n",
      " 4   session         4490 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 175.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "neu    1708\n",
       "ang    1103\n",
       "sad    1084\n",
       "hap     595\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per Session:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "session\n",
       "Session3    1000\n",
       "Session1     942\n",
       "Session5     942\n",
       "Session2     813\n",
       "Session4     793\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from core.config import CONFIG\n",
    "from preprocessing.iemocap import IemocapPreprocessor\n",
    "\n",
    "# ç¡®ä¿ CONFIG å·²ç»åŠ è½½äº†é…ç½®æ–‡ä»¶\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# åˆå§‹åŒ– IemocapPreprocessorï¼Œä½¿ç”¨é…ç½®ä¸­è®­ç»ƒæ•°æ®é›†çš„è·¯å¾„\n",
    "# å‡è®¾æ‚¨æƒ³éªŒè¯è®­ç»ƒæ•°æ®çš„åŠ è½½\n",
    "iemocap_dataset_path = CONFIG.dataset_path(\"training\")\n",
    "iemocap_preprocessor = IemocapPreprocessor(iemocap_dataset_path)\n",
    "\n",
    "# ç”Ÿæˆ DataFrame\n",
    "iemocap_df = iemocap_preprocessor.generate_dataframe()\n",
    "\n",
    "# æ˜¾ç¤º DataFrame çš„å‰å‡ è¡Œ\n",
    "print(\"DataFrame head:\")\n",
    "display(iemocap_df.head())\n",
    "\n",
    "# æ‚¨è¿˜å¯ä»¥æ‰“å°ä¸€äº›å…³äº DataFrame çš„ä¿¡æ¯æ¥è¿›ä¸€æ­¥éªŒè¯\n",
    "print(\"\\nDataFrame Info:\")\n",
    "iemocap_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(iemocap_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per Session:\")\n",
    "display(iemocap_df['session'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823ad63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>hap</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>neu</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_SAD_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>sad</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_IEO_ANG_HI.wav</td>\n",
       "      <td>It's eleven o'clock</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path       audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_ANG_XX.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_HAP_XX.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_NEU_XX.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_SAD_XX.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_IEO_ANG_HI.wav   \n",
       "\n",
       "                    text emotion speaker  \n",
       "0  Don't forget a jacket     ang    1001  \n",
       "1  Don't forget a jacket     hap    1001  \n",
       "2  Don't forget a jacket     neu    1001  \n",
       "3  Don't forget a jacket     sad    1001  \n",
       "4    It's eleven o'clock     ang    1001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4900 entries, 0 to 4899\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4900 non-null   object\n",
      " 1   audio_filename  4900 non-null   object\n",
      " 2   text            4900 non-null   object\n",
      " 3   emotion         4900 non-null   object\n",
      " 4   speaker         4900 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 191.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "ang    1271\n",
       "hap    1271\n",
       "sad    1271\n",
       "neu    1087\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "1001    54\n",
       "1047    54\n",
       "1067    54\n",
       "1066    54\n",
       "1065    54\n",
       "        ..\n",
       "1076    53\n",
       "1002    53\n",
       "1009    50\n",
       "1008    50\n",
       "1019    50\n",
       "Name: count, Length: 91, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from preprocessing.cremad import CremaDPreprocessor\n",
    "\n",
    "# å‡è®¾æ‚¨æƒ³éªŒè¯è®­ç»ƒæ•°æ®çš„åŠ è½½\n",
    "cremad_dataset_path = CONFIG.dataset_path(\"evaluation\")\n",
    "cremad_preprocessor = CremaDPreprocessor(cremad_dataset_path)\n",
    "\n",
    "# ç”Ÿæˆ DataFrame\n",
    "cremad_df = cremad_preprocessor.generate_dataframe()\n",
    "\n",
    "# æ˜¾ç¤º DataFrame çš„å‰å‡ è¡Œ\n",
    "print(\"DataFrame head:\")\n",
    "display(cremad_df.head())\n",
    "\n",
    "# æ‚¨è¿˜å¯ä»¥æ‰“å°ä¸€äº›å…³äº DataFrame çš„ä¿¡æ¯æ¥è¿›ä¸€æ­¥éªŒè¯\n",
    "print(\"\\nDataFrame Info:\")\n",
    "cremad_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(cremad_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per speaker:\")\n",
    "display(cremad_df['speaker'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c225ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing models and config ---\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: IEMOCAP\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'iemocap_raw.pkl' ---\n",
      "[INFO] Using IemocapPreprocessor for dataset: IEMOCAP\n",
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\\iemocap_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for IEMOCAP.\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: CREMA-D\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'crema-d_raw.pkl' ---\n",
      "[INFO] Using CremaDPreprocessor for dataset: CREMA-D\n",
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\\crema-d_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for CREMA-D.\n",
      "\n",
      "====================\n",
      "--- All processing complete! ---\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from audio.extractor import WavLMEmotionExtractor\n",
    "from core.config import CONFIG\n",
    "from scripts.preprocess_data import process_raw_data_to_pickle\n",
    "\n",
    "\n",
    "def run_preprocessing_pipeline(dataset_name: str):\n",
    "    \"\"\"\n",
    "    ä¸ºä¸€ä¸ªæŒ‡å®šçš„æ•°æ®é›†å®Œæ•´åœ°æ‰§è¡Œæ•°æ®é¢„å¤„ç†çš„ä¸‰ä¸ªæ­¥éª¤ã€‚\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): æ•°æ®é›†çš„åç§° (ä» CONFIG ä¸­è·å–)ã€‚\n",
    "        audio_extractor: åˆå§‹åŒ–åçš„éŸ³é¢‘ç‰¹å¾æå–å™¨ã€‚\n",
    "        text_tokenizer: åˆå§‹åŒ–åçš„æ–‡æœ¬åˆ†è¯å™¨ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20}\\n[START] Processing dataset: {dataset_name}\\n{'='*20}\")\n",
    "\n",
    "    # 1. åŠ¨æ€ç”Ÿæˆæ–‡ä»¶åï¼Œé¿å…ç¡¬ç¼–ç \n",
    "    # ä¾‹å¦‚ä» \"IEMOCAP_full_release\" ç”Ÿæˆ \"iemocap\" ä½œä¸ºæ–‡ä»¶åå‰ç¼€\n",
    "    base_name = dataset_name.split('_')[0].lower() \n",
    "    raw_file = f\"{base_name}_raw.pkl\"\n",
    "    audio_file = f\"{base_name}_audio_features.pkl\"\n",
    "    text_file = f\"{base_name}_text_tokens.pkl\"\n",
    "\n",
    "    # 2. æŒ‰é¡ºåºæ‰§è¡Œæ•°æ®å¤„ç†æµç¨‹\n",
    "    print(f\"--- Step 1: Processing raw data to '{raw_file}' ---\")\n",
    "    process_raw_data_to_pickle(dataset_name, raw_file)\n",
    "\n",
    "\n",
    "    print(f\"\\n[SUCCESS] Finished processing for {dataset_name}.\")\n",
    "\n",
    "\n",
    "# --- ä¸»æ‰§è¡Œè„šæœ¬ ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. é›†ä¸­è¿›è¡Œåˆå§‹åŒ–\n",
    "    print(\"--- Initializing models and config ---\")\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    # 2. å®šä¹‰éœ€è¦å¤„ç†çš„æ•°æ®é›†åˆ—è¡¨\n",
    "    datasets_to_process = [\n",
    "        CONFIG.training_dataset_name(),\n",
    "        CONFIG.evaluation_dataset_name()\n",
    "    ]\n",
    "\n",
    "    # 3. å¾ªç¯è°ƒç”¨å¤„ç†æµç¨‹\n",
    "    for name in datasets_to_process:\n",
    "        run_preprocessing_pipeline(name)\n",
    "\n",
    "    print(f\"\\n{'='*20}\\n--- All processing complete! ---\\n{'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5c8c4",
   "metadata": {},
   "source": [
    "## å®ä¾‹åŒ–æ•°æ®é›†\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b09ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½ IEMOCAP æ•°æ®é›†ç”¨äºè®­ç»ƒ...\n",
      "--- æ­£åœ¨ä¸ºæ•°æ®é›† 'IEMOCAP' å‡†å¤‡Dataloaders ---\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½éŸ³é¢‘ç‰¹å¾: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_audio_features.pkl\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ–‡æœ¬Tokens: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_text_tokens.pkl\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- è®­ç»ƒå’ŒéªŒè¯æµç¨‹ ---\n",
    "print(\"åŠ è½½ IEMOCAP æ•°æ®é›†ç”¨äºè®­ç»ƒ...\")\n",
    "# åªéœ€ä¸€è¡Œä»£ç ï¼Œå³å¯è·å–è®­ç»ƒå’ŒéªŒè¯æ‰€éœ€çš„æ‰€æœ‰ dataloader\n",
    "iemocap_loaders = get_dataloaders(CONFIG.training_dataset_name())\n",
    "train_loader = iemocap_loaders['train']\n",
    "validation_loader = iemocap_loaders['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd0b9d",
   "metadata": {},
   "source": [
    "### train baseline modelï¼ˆNewï¼‰\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a66ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­£åœ¨åŠ è½½ 'IEMOCAP' æ•°æ®é›†ç”¨äºè®­ç»ƒå’ŒéªŒè¯ ---\n",
      "--- æ­£åœ¨ä¸ºæ•°æ®é›† 'IEMOCAP' å‡†å¤‡Dataloaders ---\n",
      "[INFO] ä½¿ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½éŸ³é¢‘ç‰¹å¾: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_audio_features.pkl\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ–‡æœ¬Tokens: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_text_tokens.pkl\n",
      "[INFO] ä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æ•°æ®æ•´ç†å™¨\n",
      "[INFO] å·²æ¸…ç†åŠ è½½è¿‡ç¨‹ä¸­çš„ä¸´æ—¶å†…å­˜\n",
      "\n",
      "--- æ­£åœ¨åŠ è½½ 'CREMA-D' æ•°æ®é›†ç”¨äºé›¶æ ·æœ¬è¯„ä¼° ---\n",
      "--- æ­£åœ¨ä¸ºæ•°æ®é›† 'CREMA-D' å‡†å¤‡Dataloaders ---\n",
      "[INFO] ä½¿ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½éŸ³é¢‘ç‰¹å¾: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_audio_features.pkl\n",
      "[INFO] å·²ä»ä»¥ä¸‹è·¯å¾„åŠ è½½æ–‡æœ¬Tokens: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_text_tokens.pkl\n",
      "[INFO] ä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„æ•°æ®æ•´ç†å™¨\n",
      "[INFO] å·²æ¸…ç†åŠ è½½è¿‡ç¨‹ä¸­çš„ä¸´æ—¶å†…å­˜\n",
      "\n",
      "--- åˆå§‹åŒ–åŸºçº¿æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['projector.bias', 'projector.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10\\cuda\\CUDACachingAllocator.cpp:803.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\wavlm\\modeling_wavlm.py:1434: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ç‰¹å¾æå–å±‚å·²å†»ç»“ï¼Œä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: 8\n",
      "\n",
      "--- å¼€å§‹åœ¨ IEMOCAP ä¸Šè®­ç»ƒåŸºçº¿æ¨¡å‹ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Rerun the training cell\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import MemoryOptimizedAudioBaselineTrainer\n",
    "\n",
    "# è®¾ç½®CUDAå†…å­˜ä¼˜åŒ–\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- è®­ç»ƒå’ŒéªŒè¯æµç¨‹ ---\n",
    "training_dataset_name = CONFIG.training_dataset_name()\n",
    "print(f\"\\n--- æ­£åœ¨åŠ è½½ '{training_dataset_name}' æ•°æ®é›†ç”¨äºè®­ç»ƒå’ŒéªŒè¯ ---\")\n",
    "\n",
    "# ä½¿ç”¨å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬çš„dataloadersï¼ˆè¿™æ˜¯ä¸»è¦çš„æ”¹åŠ¨ï¼‰\n",
    "try:\n",
    "    # ä½¿ç”¨å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name, use_memory_optimization=True)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "    \n",
    "    # --- é›¶æ ·æœ¬è¯„ä¼°æµç¨‹ (åœ¨ CREMA-D ä¸Š) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- æ­£åœ¨åŠ è½½ '{evaluation_dataset_name}' æ•°æ®é›†ç”¨äºé›¶æ ·æœ¬è¯„ä¼° ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name, use_memory_optimization=True)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    # --- å®ä¾‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\n",
    "    print(\"\\n--- åˆå§‹åŒ–åŸºçº¿æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\")\n",
    "    \n",
    "    # æ¸…ç†å†…å­˜ååˆ›å»ºæ¨¡å‹\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # è·å–æƒ…æ„Ÿæ ‡ç­¾\n",
    "    iemocap_emotions = CONFIG.dataset_emotions(training_dataset_name)\n",
    "    num_labels = len(iemocap_emotions)\n",
    "    \n",
    "    # åˆ›å»ºæ¨¡å‹ï¼ˆè€ƒè™‘ä½¿ç”¨æ›´å°çš„batch_size)\n",
    "    baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "    \n",
    "    # ä½¿ç”¨å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬çš„è®­ç»ƒå™¨\n",
    "    baseline_trainer = MemoryOptimizedAudioBaselineTrainer(  # ä½¿ç”¨æ–°çš„è®­ç»ƒå™¨\n",
    "        model=baseline_model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate() * 4,  # ç”±äºæ¢¯åº¦ç´¯ç§¯ï¼Œéœ€è¦è°ƒæ•´å­¦ä¹ ç‡\n",
    "        optimizer_type=CONFIG.optimizer_type(),\n",
    "        gradient_accumulation_steps=8  # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "    )\n",
    "    \n",
    "    # --- æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹ ---\n",
    "    print(\"\\n--- å¼€å§‹åœ¨ IEMOCAP ä¸Šè®­ç»ƒåŸºçº¿æ¨¡å‹ ---\")\n",
    "    baseline_trainer.train(train_loader)\n",
    "    \n",
    "    # --- æ­¥éª¤ 4: åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼° ---\n",
    "    print(\"\\n--- åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ ---\")\n",
    "    baseline_trainer.eval(validation_loader, labels=iemocap_emotions)\n",
    "    \n",
    "    # --- æ­¥éª¤ 5: åœ¨ CREMA-D æµ‹è¯•é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° ---\n",
    "    print(\"\\n--- åœ¨ CREMA-D æµ‹è¯•é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° ---\")\n",
    "    cremad_emotions = CONFIG.dataset_emotions(evaluation_dataset_name)\n",
    "    baseline_trainer.eval(evaluation_loader, labels=cremad_emotions)\n",
    "    \n",
    "    print(\"\\n--- åŸºçº¿æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼ ---\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\n[ERROR] CUDAå†…å­˜ä¸è¶³: {e}\")\n",
    "    print(\"å»ºè®®:\")\n",
    "    print(\"1. è¿›ä¸€æ­¥å‡å°batch_sizeåˆ°1\")\n",
    "    print(\"2. å‡å°‘éŸ³é¢‘æœ€å¤§é•¿åº¦\")\n",
    "    print(\"3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹variant\")\n",
    "    print(\"4. é‡å¯è¿è¡Œæ—¶æ¸…ç†å†…å­˜\")\n",
    "    \n",
    "    # æ¸…ç†å†…å­˜\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697ad0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Check PyTorch and CUDA ---\n",
      "Python Version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "PyTorch Version: 2.1.2+cu121\n",
      "Is CUDA available: True\n",
      "\n",
      "--- 2. Get GPU Device Information ---\n",
      "Default CUDA device: cuda:0\n",
      "GPU Name: NVIDIA L40S-6Q\n",
      "PyTorch compiled with CUDA version: 12.1\n",
      "\n",
      "--- 3. Test Data Transfer Between CPU and GPU ---\n",
      "a. Tensor created on the CPU: tensor([1, 2, 3])\n",
      "   - Device: cpu\n",
      "\n",
      "[ERROR] Failed to move data to GPU: CUDA error: operation not supported\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"--- 1. Check PyTorch and CUDA ---\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available: {is_cuda_available}\")\n",
    "\n",
    "if not is_cuda_available:\n",
    "    print(\"\\n[ERROR] PyTorch could not detect CUDA. Please check your NVIDIA driver and PyTorch installation.\")\n",
    "    # If CUDA is not available, exit the script\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 2. Get GPU Device Information ---\")\n",
    "# Get the default CUDA device (usually GPU 0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Default CUDA device: {device}\")\n",
    "\n",
    "# Print the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "# Print the CUDA version PyTorch was compiled with\n",
    "torch_cuda_version = torch.version.cuda\n",
    "print(f\"PyTorch compiled with CUDA version: {torch_cuda_version}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. Test Data Transfer Between CPU and GPU ---\")\n",
    "# a. Create a tensor on the CPU\n",
    "cpu_tensor = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(f\"a. Tensor created on the CPU: {cpu_tensor}\")\n",
    "print(f\"   - Device: {cpu_tensor.device}\")\n",
    "\n",
    "# b. Try to move the tensor to the GPU\n",
    "try:\n",
    "    gpu_tensor = cpu_tensor.to(device)\n",
    "    print(f\"\\nb. Successfully moved tensor to GPU: {gpu_tensor}\")\n",
    "    print(f\"   - Device: {gpu_tensor.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move data to GPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. Test Computation on GPU ---\")\n",
    "# a. Create two tensors on the GPU for computation\n",
    "try:\n",
    "    a = torch.randn(3, 3).to(device)\n",
    "    b = torch.randn(3, 3).to(device)\n",
    "    print(f\"a. Created two 3x3 random tensors on the GPU.\")\n",
    "    print(f\"   - Tensor a device: {a.device}\")\n",
    "    print(f\"   - Tensor b device: {b.device}\")\n",
    "\n",
    "    # b. Perform matrix multiplication on the GPU\n",
    "    print(\"\\nb. Performing matrix multiplication on GPU (c = a * b)...\")\n",
    "    c = torch.matmul(a, b)\n",
    "    print(f\"   - Result c device: {c.device}\")\n",
    "    print(f\"   - Computation successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Computation on GPU failed: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 5. Test Moving Result Back to CPU ---\")\n",
    "# a. Move the computation result from GPU back to CPU\n",
    "try:\n",
    "    result_cpu_tensor = c.cpu()\n",
    "    print(\"a. Successfully moved the computation result back to the CPU.\")\n",
    "    print(f\"   - Device: {result_cpu_tensor.device}\")\n",
    "    print(\"\\nComputation result:\")\n",
    "    print(result_cpu_tensor)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move result back to CPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- All tests completed ---\")\n",
    "print(\"[SUCCESS] Your PyTorch and CUDA environment is configured correctly, and they can communicate and perform computations normally!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9cce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- å¢å¼ºå‹ PyTorch & NVIDIA vGPU ç¯å¢ƒè¯Šæ–­å·¥å…· ---\n",
      "è¯Šæ–­å¼€å§‹æ—¶é—´: 2025-08-19 17:11:45\n",
      "æ“ä½œç³»ç»Ÿ: Windows 10\n",
      "\n",
      "--- 1. ç³»ç»Ÿå’Œé©±åŠ¨ä¿¡æ¯ ---\n",
      "ğŸ Python ç‰ˆæœ¬: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "ğŸ”¥ PyTorch ç‰ˆæœ¬: 2.1.2+cu121\n",
      "\n",
      "... æ­£åœ¨è¿è¡Œ `nvidia-smi` è·å–é©±åŠ¨ä¿¡æ¯...\n",
      "âœ… `nvidia-smi` å‘½ä»¤æˆåŠŸæ‰§è¡Œã€‚\n",
      "   - GPU å‹å·: NVIDIA L40S-6Q               WDDM\n",
      "   - é©±åŠ¨ç‰ˆæœ¬: 538.95\n",
      "   - é©±åŠ¨æ”¯æŒçš„æœ€é«˜ CUDA ç‰ˆæœ¬: 12.2\n",
      "\n",
      "   - [å…³é”®ä¿¡æ¯] æ£€æµ‹åˆ° vGPU ç¯å¢ƒï¼å°†æ‰§è¡Œ vGPU è®¸å¯è¯è¯Šæ–­ã€‚\n",
      "\n",
      "--- 2. vGPU è®¸å¯è¯æœåŠ¡è¯Šæ–­ ---\n",
      "   - æ­£åœ¨æ£€æŸ¥ Windows æœåŠ¡: 'NVIDIA Display Container LS'...\n",
      "   - æœåŠ¡çŠ¶æ€: æœªæ‰¾åˆ°æˆ–æœªå®‰è£… (Not Found/Installed)\n",
      "   - [ä¸¥é‡] NVIDIA è®¸å¯è¯æœåŠ¡æœªè¿è¡Œï¼è¿™æ˜¯å¯¼è‡´åŠŸèƒ½å—é™çš„ç›´æ¥åŸå› ã€‚\n",
      "   - è§£å†³æ–¹æ¡ˆ: è¯·å¯åŠ¨è¯¥æœåŠ¡ã€‚Windows: `net start nvdisplay.container.service`, Linux: `sudo systemctl start nvidia-gridd`\n",
      "ğŸ“„ æ­£åœ¨æ£€æŸ¥ vGPU æ—¥å¿—æ–‡ä»¶: C:\\Users\\Public\\Documents\\NvidiaLogging\\Log.NVDisplay.Container.exe.log\n",
      "   - [ä¸¥é‡] åœ¨æ—¥å¿—ä¸­å‘ç°å¯èƒ½çš„è®¸å¯è¯é”™è¯¯: è·å–è®¸å¯è¯å¤±è´¥\n",
      "   - è¯Šæ–­: GPU å¯èƒ½å› æ— æ³•è·å–æœ‰æ•ˆè®¸å¯è¯è€Œè¢«é™åˆ¶äº†è®¡ç®—åŠŸèƒ½ã€‚è¯·æ£€æŸ¥æ‚¨çš„è®¸å¯è¯æœåŠ¡å™¨åœ°å€ã€ç½‘ç»œè¿æ¥å’Œå®¢æˆ·ç«¯é…ç½®ã€‚\n",
      "\n",
      "--- 3. PyTorch CUDA éªŒè¯ ---\n",
      "ğŸ” PyTorch æ˜¯å¦èƒ½æ‰¾åˆ° CUDA: True\n",
      "âœ… PyTorch å¯ä»¥è®¿é—® CUDAã€‚\n",
      "   - PyTorch ç¼–è¯‘æ‰€ç”¨ CUDA ç‰ˆæœ¬: 12.1\n",
      "   - å¯ç”¨ GPU æ•°é‡: 1\n",
      "\n",
      "--- 4. CUDA æ ¸å¿ƒæ“ä½œæµ‹è¯• ---\n",
      "a. å°è¯•åœ¨ GPU (cuda:0) ä¸Šåˆ›å»ºå¼ é‡...\n",
      "\n",
      "[CRITICAL ERROR] åœ¨æ‰§è¡Œ CUDA æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯: CUDA error: operation not supported\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "\n",
      "[vGPU è¯Šæ–­]\n",
      "åœ¨ vGPU ç¯å¢ƒä¸‹ï¼Œæ­¤é”™è¯¯ï¼ˆç‰¹åˆ«æ˜¯ 'operation not supported' æˆ– 'initialization error'ï¼‰ææœ‰å¯èƒ½æ˜¯ç”± **è®¸å¯è¯é—®é¢˜** é€ æˆçš„ã€‚\n",
      "å³ä½¿æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œè®¸å¯è¯æœåŠ¡å™¨ä¹Ÿå¯èƒ½æ— æ³•è®¿é—®æˆ–æ²¡æœ‰å¯ç”¨çš„è®¸å¯è¯ã€‚\n",
      "è¯·é‡ç‚¹æ£€æŸ¥ç¬¬ 2 éƒ¨åˆ†çš„æ—¥å¿—åˆ†æç»“æœï¼Œå¹¶è”ç³»æ‚¨çš„ç³»ç»Ÿç®¡ç†å‘˜ç¡®è®¤è®¸å¯è¯é…ç½®ã€‚\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import platform\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- è¾…åŠ©å‡½æ•° ---\n",
    "def run_command(command, shell=False):\n",
    "    \"\"\"æ‰§è¡Œä¸€ä¸ª shell å‘½ä»¤å¹¶è¿”å›å…¶è¾“å‡ºã€‚\"\"\"\n",
    "    try:\n",
    "        # åœ¨ Windows ä¸Šéšè—å‘½ä»¤è¡Œçª—å£\n",
    "        startupinfo = None\n",
    "        if platform.system() == \"Windows\":\n",
    "            startupinfo = subprocess.STARTUPINFO()\n",
    "            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n",
    "\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            encoding='utf-8',\n",
    "            errors='ignore',\n",
    "            check=True,\n",
    "            shell=shell,\n",
    "            startupinfo=startupinfo\n",
    "        )\n",
    "        return result.stdout.strip()\n",
    "    except (FileNotFoundError, subprocess.CalledProcessError) as e:\n",
    "        return f\"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}\"\n",
    "\n",
    "def check_windows_service(service_name):\n",
    "    \"\"\"åœ¨ Windows ä¸Šæ£€æŸ¥æŒ‡å®šæœåŠ¡çš„çŠ¶æ€ã€‚\"\"\"\n",
    "    output = run_command(['sc', 'query', service_name])\n",
    "    if \"STATE\" in output and \"RUNNING\" in output:\n",
    "        return \"æ­£åœ¨è¿è¡Œ (Running)\"\n",
    "    elif \"FAILED\" in output or \"1060\" in output: # 1060: The specified service does not exist\n",
    "        return \"æœªæ‰¾åˆ°æˆ–æœªå®‰è£… (Not Found/Installed)\"\n",
    "    else:\n",
    "        return \"å·²åœæ­¢ (Stopped)\"\n",
    "\n",
    "def check_linux_service(service_name):\n",
    "    \"\"\"åœ¨ Linux ä¸Šæ£€æŸ¥æŒ‡å®šæœåŠ¡çš„çŠ¶æ€ã€‚\"\"\"\n",
    "    output = run_command(f\"systemctl is-active {service_name}\")\n",
    "    if \"active\" in output:\n",
    "        return \"æ­£åœ¨è¿è¡Œ (Active)\"\n",
    "    elif \"inactive\" in output:\n",
    "        return \"å·²åœæ­¢ (Inactive)\"\n",
    "    else:\n",
    "        return \"æœªæ‰¾åˆ°æˆ–çŠ¶æ€æœªçŸ¥ (Not Found or Unknown)\"\n",
    "\n",
    "def analyze_vgpu_logs():\n",
    "    \"\"\"æŸ¥æ‰¾å¹¶åˆ†æ NVIDIA vGPU è®¸å¯è¯æ—¥å¿—æ–‡ä»¶ã€‚\"\"\"\n",
    "    log_path = \"\"\n",
    "    system = platform.system()\n",
    "    if system == \"Windows\":\n",
    "        log_path = \"C:\\\\Users\\\\Public\\\\Documents\\\\NvidiaLogging\\\\Log.NVDisplay.Container.exe.log\"\n",
    "    elif system == \"Linux\":\n",
    "        log_path = \"/var/log/nvidia/gridd.log\"\n",
    "\n",
    "    print(f\"ğŸ“„ æ­£åœ¨æ£€æŸ¥ vGPU æ—¥å¿—æ–‡ä»¶: {log_path}\")\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        print(\"   - [è­¦å‘Š] æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶ã€‚å¯èƒ½æœåŠ¡ä»æœªè¿è¡Œè¿‡ï¼Œæˆ–æ—¥å¿—åœ¨å…¶ä»–ä½ç½®ã€‚\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with open(log_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            # åªè¯»å–æœ€å 100 è¡Œä»¥æé«˜æ•ˆç‡\n",
    "            log_content = f.readlines()[-100:]\n",
    "            log_content = \"\".join(log_content)\n",
    "\n",
    "        # å¸¸è§çš„è®¸å¯è¯é”™è¯¯å…³é”®è¯\n",
    "        error_keywords = {\n",
    "            \"Failed to acquire license\": \"è·å–è®¸å¯è¯å¤±è´¥\",\n",
    "            \"Failed to connect to license server\": \"è¿æ¥åˆ°è®¸å¯è¯æœåŠ¡å™¨å¤±è´¥\",\n",
    "            \"could not connect to\": \"æ— æ³•è¿æ¥åˆ°\",\n",
    "            \"Connection refused\": \"è¿æ¥è¢«æ‹’ç»\",\n",
    "            \"License request failed\": \"è®¸å¯è¯è¯·æ±‚å¤±è´¥\",\n",
    "            \"unlicensed\": \"æœªæˆæƒçŠ¶æ€\",\n",
    "            \"terminated\": \"å·²ç»ˆæ­¢\"\n",
    "        }\n",
    "\n",
    "        found_errors = []\n",
    "        for key, value in error_keywords.items():\n",
    "            if re.search(key, log_content, re.IGNORECASE):\n",
    "                found_errors.append(value)\n",
    "        \n",
    "        if found_errors:\n",
    "            print(f\"   - [ä¸¥é‡] åœ¨æ—¥å¿—ä¸­å‘ç°å¯èƒ½çš„è®¸å¯è¯é”™è¯¯: {', '.join(found_errors)}\")\n",
    "            print(\"   - è¯Šæ–­: GPU å¯èƒ½å› æ— æ³•è·å–æœ‰æ•ˆè®¸å¯è¯è€Œè¢«é™åˆ¶äº†è®¡ç®—åŠŸèƒ½ã€‚è¯·æ£€æŸ¥æ‚¨çš„è®¸å¯è¯æœåŠ¡å™¨åœ°å€ã€ç½‘ç»œè¿æ¥å’Œå®¢æˆ·ç«¯é…ç½®ã€‚\")\n",
    "        else:\n",
    "            print(\"   - [ä¿¡æ¯] åœ¨æœ€è¿‘çš„æ—¥å¿—ä¸­æœªå‘ç°æ˜æ˜¾çš„è®¸å¯è¯é”™è¯¯ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   - [é”™è¯¯] è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "\n",
    "# --- ä¸»è¯Šæ–­æµç¨‹ ---\n",
    "print(\"--- å¢å¼ºå‹ PyTorch & NVIDIA vGPU ç¯å¢ƒè¯Šæ–­å·¥å…· ---\")\n",
    "print(f\"è¯Šæ–­å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}\")\n",
    "\n",
    "print(\"\\n--- 1. ç³»ç»Ÿå’Œé©±åŠ¨ä¿¡æ¯ ---\")\n",
    "print(f\"ğŸ Python ç‰ˆæœ¬: {sys.version.splitlines()[0]}\")\n",
    "print(f\"ğŸ”¥ PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "\n",
    "print(\"\\n... æ­£åœ¨è¿è¡Œ `nvidia-smi` è·å–é©±åŠ¨ä¿¡æ¯...\")\n",
    "nvidia_smi_output = run_command(['nvidia-smi'])\n",
    "if \"å‘½ä»¤æ‰§è¡Œå¤±è´¥\" not in nvidia_smi_output:\n",
    "    print(\"âœ… `nvidia-smi` å‘½ä»¤æˆåŠŸæ‰§è¡Œã€‚\")\n",
    "    driver_match = re.search(r\"Driver Version: ([\\d\\.]+)\", nvidia_smi_output)\n",
    "    cuda_match = re.search(r\"CUDA Version: ([\\d\\.]+)\", nvidia_smi_output)\n",
    "    gpu_name_match = re.search(r\"\\d+\\s+(NVIDIA\\s[\\w\\s-]+)\\s+\", nvidia_smi_output)\n",
    "    \n",
    "    driver_version = driver_match.group(1) if driver_match else \"æœªæ£€æµ‹åˆ°\"\n",
    "    driver_cuda_version = cuda_match.group(1) if cuda_match else \"æœªæ£€æµ‹åˆ°\"\n",
    "    gpu_name = gpu_name_match.group(1).strip() if gpu_name_match else \"æœªæ£€æµ‹åˆ°\"\n",
    "    \n",
    "    print(f\"   - GPU å‹å·: {gpu_name}\")\n",
    "    print(f\"   - é©±åŠ¨ç‰ˆæœ¬: {driver_version}\")\n",
    "    print(f\"   - é©±åŠ¨æ”¯æŒçš„æœ€é«˜ CUDA ç‰ˆæœ¬: {driver_cuda_version}\")\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦ä¸º vGPU ç¯å¢ƒ\n",
    "    if \"vGPU\" in nvidia_smi_output or re.search(r\"\\w+-\\d+Q\", gpu_name):\n",
    "        print(\"\\n   - [å…³é”®ä¿¡æ¯] æ£€æµ‹åˆ° vGPU ç¯å¢ƒï¼å°†æ‰§è¡Œ vGPU è®¸å¯è¯è¯Šæ–­ã€‚\")\n",
    "        is_vgpu = True\n",
    "    else:\n",
    "        is_vgpu = False\n",
    "else:\n",
    "    print(\"\\n[CRITICAL ERROR] `nvidia-smi` å‘½ä»¤æ‰§è¡Œå¤±è´¥ã€‚\")\n",
    "    print(\"æ— æ³•éªŒè¯ NVIDIA é©±åŠ¨å®‰è£…ã€‚è¯·ç¡®ä¿é©±åŠ¨å·²æ­£ç¡®å®‰è£…ä¸” `nvidia-smi` åœ¨ç³»ç»Ÿ PATH ä¸­ã€‚\")\n",
    "    sys.exit()\n",
    "\n",
    "# --- vGPU è®¸å¯è¯è¯Šæ–­éƒ¨åˆ† ---\n",
    "if is_vgpu:\n",
    "    print(\"\\n--- 2. vGPU è®¸å¯è¯æœåŠ¡è¯Šæ–­ ---\")\n",
    "    system = platform.system()\n",
    "    service_status = \"\"\n",
    "    if system == \"Windows\":\n",
    "        service_name = \"NVIDIA Display Container LS\"\n",
    "        print(f\"   - æ­£åœ¨æ£€æŸ¥ Windows æœåŠ¡: '{service_name}'...\")\n",
    "        service_status = check_windows_service(\"nvdisplay.container.service\")\n",
    "        print(f\"   - æœåŠ¡çŠ¶æ€: {service_status}\")\n",
    "    elif system == \"Linux\":\n",
    "        service_name = \"nvidia-gridd\"\n",
    "        print(f\"   - æ­£åœ¨æ£€æŸ¥ Linux æœåŠ¡: '{service_name}'...\")\n",
    "        service_status = check_linux_service(service_name)\n",
    "        print(f\"   - æœåŠ¡çŠ¶æ€: {service_status}\")\n",
    "    \n",
    "    if \"æ­£åœ¨è¿è¡Œ\" not in service_status and \"Active\" not in service_status:\n",
    "        print(\"   - [ä¸¥é‡] NVIDIA è®¸å¯è¯æœåŠ¡æœªè¿è¡Œï¼è¿™æ˜¯å¯¼è‡´åŠŸèƒ½å—é™çš„ç›´æ¥åŸå› ã€‚\")\n",
    "        print(\"   - è§£å†³æ–¹æ¡ˆ: è¯·å¯åŠ¨è¯¥æœåŠ¡ã€‚Windows: `net start nvdisplay.container.service`, Linux: `sudo systemctl start nvidia-gridd`\")\n",
    "    else:\n",
    "        print(\"   - âœ… æœåŠ¡æ­£åœ¨è¿è¡Œã€‚\")\n",
    "\n",
    "    analyze_vgpu_logs()\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. PyTorch CUDA éªŒè¯ ---\")\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"ğŸ” PyTorch æ˜¯å¦èƒ½æ‰¾åˆ° CUDA: {is_cuda_available}\")\n",
    "\n",
    "if not is_cuda_available:\n",
    "    print(\"\\n[CRITICAL ERROR] PyTorch æŠ¥å‘Š CUDA ä¸å¯ç”¨ã€‚\")\n",
    "    print(\"å¸¸è§åŸå› :\")\n",
    "    print(\"  1. æ‚¨å¯èƒ½å®‰è£…äº†ä»… CPU ç‰ˆæœ¬çš„ PyTorchã€‚è¯·è®¿é—®å®˜ç½‘ (pytorch.org) è·å–æ­£ç¡®çš„ CUDA ç‰ˆæœ¬å®‰è£…å‘½ä»¤ã€‚\")\n",
    "    print(\"  2. NVIDIA é©±åŠ¨ä¸ PyTorch çš„ CUDA å·¥å…·åŒ…ç‰ˆæœ¬ä¸å…¼å®¹ã€‚\")\n",
    "    if is_vgpu:\n",
    "        print(\"  3. (vGPU ç¯å¢ƒ) è®¸å¯è¯è·å–å¤±è´¥å¯¼è‡´ GPU è®¡ç®—åŠŸèƒ½è¢«ç¦ç”¨ï¼ŒPyTorch æ— æ³•è®¿é—®ã€‚\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"âœ… PyTorch å¯ä»¥è®¿é—® CUDAã€‚\")\n",
    "print(f\"   - PyTorch ç¼–è¯‘æ‰€ç”¨ CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "device_count = torch.cuda.device_count()\n",
    "print(f\"   - å¯ç”¨ GPU æ•°é‡: {device_count}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. CUDA æ ¸å¿ƒæ“ä½œæµ‹è¯• ---\")\n",
    "if device_count > 0:\n",
    "    try:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"a. å°è¯•åœ¨ GPU (cuda:0) ä¸Šåˆ›å»ºå¼ é‡...\")\n",
    "        a = torch.randn(3, 3, device=device)\n",
    "        print(f\"   - âœ… æˆåŠŸåœ¨ {a.device} ({gpu_name}) ä¸Šåˆ›å»ºå¼ é‡ã€‚\")\n",
    "\n",
    "        print(f\"b. å°è¯•åœ¨ GPU ä¸Šæ‰§è¡ŒçŸ©é˜µä¹˜æ³•...\")\n",
    "        b = torch.randn(3, 3, device=device)\n",
    "        c = torch.matmul(a, b)\n",
    "        print(f\"   - âœ… è®¡ç®—æˆåŠŸï¼Œç»“æœä½äº {c.device}ã€‚\")\n",
    "        \n",
    "        print(f\"c. å°è¯•å°†ç»“æœç§»å› CPU...\")\n",
    "        result_cpu = c.cpu()\n",
    "        print(f\"   - âœ… æˆåŠŸå°†ç»“æœç§»å› {result_cpu.device}ã€‚\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[CRITICAL ERROR] åœ¨æ‰§è¡Œ CUDA æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        if is_vgpu:\n",
    "            print(\"\\n[vGPU è¯Šæ–­]\")\n",
    "            print(\"åœ¨ vGPU ç¯å¢ƒä¸‹ï¼Œæ­¤é”™è¯¯ï¼ˆç‰¹åˆ«æ˜¯ 'operation not supported' æˆ– 'initialization error'ï¼‰ææœ‰å¯èƒ½æ˜¯ç”± **è®¸å¯è¯é—®é¢˜** é€ æˆçš„ã€‚\")\n",
    "            print(\"å³ä½¿æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œè®¸å¯è¯æœåŠ¡å™¨ä¹Ÿå¯èƒ½æ— æ³•è®¿é—®æˆ–æ²¡æœ‰å¯ç”¨çš„è®¸å¯è¯ã€‚\")\n",
    "            print(\"è¯·é‡ç‚¹æ£€æŸ¥ç¬¬ 2 éƒ¨åˆ†çš„æ—¥å¿—åˆ†æç»“æœï¼Œå¹¶è”ç³»æ‚¨çš„ç³»ç»Ÿç®¡ç†å‘˜ç¡®è®¤è®¸å¯è¯é…ç½®ã€‚\")\n",
    "        else:\n",
    "            print(\"\\n[é€šç”¨è¯Šæ–­]\")\n",
    "            print(\"æ­¤é”™è¯¯å¯èƒ½ç”±é©±åŠ¨ä¸ç¨³å®šã€ç¡¬ä»¶é—®é¢˜æˆ– PyTorch ä¸é©±åŠ¨çš„æ·±å±‚ä¸å…¼å®¹å¯¼è‡´ã€‚å»ºè®®é¦–å…ˆå°è¯•é‡å¯ç³»ç»Ÿå’Œæ›´æ–°é©±åŠ¨ç¨‹åºã€‚\")\n",
    "        sys.exit()\n",
    "else:\n",
    "    print(\"[è­¦å‘Š] æ²¡æœ‰å¯ç”¨çš„ GPU è®¾å¤‡è¿›è¡Œæ“ä½œæµ‹è¯•ã€‚\")\n",
    "\n",
    "\n",
    "print(\"\\n-------------------------------------------------\")\n",
    "print(\"âœ… [è¯Šæ–­å®Œæˆ] æ ¸å¿ƒ CUDA æ“ä½œæµ‹è¯•é€šè¿‡ï¼\")\n",
    "if is_vgpu:\n",
    "    print(\"æ‚¨çš„ vGPU ç¯å¢ƒä¼¼ä¹å·²ä¸º PyTorch å‡†å¤‡å°±ç»ªã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·é¦–å…ˆå…³æ³¨è®¸å¯è¯æœåŠ¡çš„çŠ¶æ€å’Œæ—¥å¿—ã€‚\")\n",
    "else:\n",
    "    print(\"æ‚¨çš„ PyTorch ç¯å¢ƒå·²æ­£ç¡®é…ç½®ï¼Œå¯ä»¥ä½¿ç”¨ NVIDIA GPUã€‚\")\n",
    "print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce7a1f",
   "metadata": {},
   "source": [
    "### ï¼ˆpassï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa1a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mget_dataloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataloaders\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineTrainer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    ä¸»å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œå£°å­¦åŸºçº¿æ¨¡å‹çš„å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ã€‚\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\audio\\trainer.py:16\u001b[39m\n\u001b[32m     13\u001b[39m torch.set_float32_matmul_precision(\u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# å¯é€‰ï¼šé¦–æ¬¡é¢„çƒ­ï¼Œé¿å…ç¬¬ä¸€æ¬¡è°ƒç”¨å¡é¡¿\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m _ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# class AudioTrainer(AbstractTrainer):\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#     def __init__(\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#         self,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# å‡è®¾ AbstractTrainer å·²ç»ä» core/trainer.py å¯¼å…¥æˆ–åœ¨notebookä¸­å®šä¹‰\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    ä¸»å‡½æ•°ï¼Œç”¨äºæ‰§è¡Œå£°å­¦åŸºçº¿æ¨¡å‹çš„å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ã€‚\n",
    "    \"\"\"\n",
    "    # 1. åŠ è½½é…ç½®æ–‡ä»¶\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "    print(f\"--- å®éªŒé…ç½®å·²åŠ è½½ ---\")\n",
    "    print(f\"ä½¿ç”¨çš„è®¾å¤‡: {device}\")\n",
    "\n",
    "    # --- è®­ç»ƒå’ŒéªŒè¯æµç¨‹ ---\n",
    "    training_dataset_name = CONFIG.training_dataset_name()\n",
    "    print(f\"\\n--- æ­£åœ¨åŠ è½½ '{training_dataset_name}' æ•°æ®é›†ç”¨äºè®­ç»ƒå’ŒéªŒè¯ ---\")\n",
    "    \n",
    "    # ä½¿ç”¨é«˜çº§å‡½æ•°è·å–è®­ç»ƒå’ŒéªŒè¯æ‰€éœ€çš„æ‰€æœ‰ dataloader\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "\n",
    "    # 3. åˆå§‹åŒ–æ¨¡å‹\n",
    "    # ä»é…ç½®ä¸­è·å–æƒ…æ„Ÿæ ‡ç­¾åˆ—è¡¨ï¼Œä»¥ç¡®å®šæ¨¡å‹çš„è¾“å‡ºç»´åº¦\n",
    "    num_labels = len(CONFIG.dataset_emotions(training_dataset_name))\n",
    "    print(f\"\\n--- æ­£åœ¨åˆå§‹åŒ– AudioBaselineModel (ç±»åˆ«æ•°: {num_labels}) ---\")\n",
    "    model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "    # 4. åˆå§‹åŒ–è®­ç»ƒå™¨\n",
    "    print(f\"--- æ­£åœ¨åˆå§‹åŒ– AudioBaselineTrainer ---\")\n",
    "    trainer = AudioBaselineTrainer(\n",
    "        model=model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate(),\n",
    "        optimizer_type=CONFIG.optimizer_type()\n",
    "    )\n",
    "\n",
    "    # 5. å¼€å§‹è®­ç»ƒ\n",
    "    trainer.train(train_loader)\n",
    "\n",
    "    # 6. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°\n",
    "    print(f\"\\n--- æ­£åœ¨ '{training_dataset_name}' çš„éªŒè¯é›†ä¸Šè¿›è¡Œè¯„ä¼° ---\")\n",
    "    trainer.eval(validation_loader, labels=CONFIG.dataset_emotions(training_dataset_name))\n",
    "\n",
    "    # --- é›¶æ ·æœ¬è¯„ä¼°æµç¨‹ (åœ¨ CREMA-D ä¸Š) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- æ­£åœ¨åŠ è½½ '{evaluation_dataset_name}' æ•°æ®é›†ç”¨äºé›¶æ ·æœ¬è¯„ä¼° ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    print(f\"\\n--- æ­£åœ¨ '{evaluation_dataset_name}' ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° ---\")\n",
    "    trainer.eval(evaluation_loader, labels=CONFIG.dataset_emotions(evaluation_dataset_name))\n",
    "    \n",
    "    print(\"\\n--- è®­ç»ƒå’Œè¯„ä¼°æµç¨‹å…¨éƒ¨å®Œæˆ ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d06293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "å·²åˆ›å»ºè®­ç»ƒé›†ï¼ŒåŒ…å« 3592 ä¸ªæ ·æœ¬ã€‚\n",
      "å·²åˆ›å»ºéªŒè¯é›†ï¼ŒåŒ…å« 898 ä¸ªæ ·æœ¬ã€‚\n",
      "å·²åˆ›å»ºè®­ç»ƒé›†ï¼ŒåŒ…å« 3920 ä¸ªæ ·æœ¬ã€‚\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- ç¬¬ 1 æ­¥: åŠ è½½å¿…è¦çš„åº“å’Œä½ çš„è‡ªå®šä¹‰ç±» ---\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, DebertaV2Tokenizer\n",
    "import os\n",
    "\n",
    "from dataloaders.dataset import CustomSERDataset\n",
    "from scripts.get_dataloaders import CustomDataCollator\n",
    "\n",
    "\n",
    "# --- ç¡®å®šè®¡ç®—è®¾å¤‡ ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- ç¬¬ 2 æ­¥: åˆå§‹åŒ–å¤„ç†å™¨å’Œåˆ†è¯å™¨ ---\n",
    "# è¿™äº›æ˜¯collatoréœ€è¦çš„ \"å·¥å…·\"\n",
    "audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG.audio_encoder_name())\n",
    "text_tokenizer = DebertaV2Tokenizer.from_pretrained(CONFIG.text_encoder_name())\n",
    "\n",
    "# --- ç¬¬ 3 æ­¥: å®ä¾‹åŒ–è®­ç»ƒæ•°æ®é›† (IEMOCAP) ---\n",
    "# æ³¨æ„ï¼šä½ éœ€è¦å…ˆè¿è¡Œä½ çš„é¢„å¤„ç†è„šæœ¬ï¼Œç”Ÿæˆç»Ÿä¸€çš„å…ƒæ•°æ®æ–‡ä»¶\n",
    "\n",
    "\n",
    "# å®šä¹‰æœ€å¤§é•¿åº¦ (ä¾‹å¦‚10ç§’)\n",
    "MAX_LEN_IN_SECONDS = 10\n",
    "max_audio_len = 16000 * MAX_LEN_IN_SECONDS\n",
    "\n",
    "iemocap_emotions = CONFIG.dataset_emotions(CONFIG.training_dataset_name())\n",
    "train_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename), # ä½¿ç”¨é…ç½®å’Œç”Ÿæˆçš„æ–‡ä»¶å\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='train',\n",
    "    max_audio_length=max_audio_len # ä¼ å…¥å‚æ•°\n",
    ")\n",
    "\n",
    "# åˆ›å»ºIEMOCAPéªŒè¯æ•°æ®é›†\n",
    "val_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename),\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='val',\n",
    "    max_audio_length=max_audio_len # ä¼ å…¥å‚æ•°\n",
    ")\n",
    "\n",
    "# --- ç¬¬ 4 æ­¥: å®ä¾‹åŒ–è¯„ä¼°æ•°æ®é›† (CREMA-D) ---\n",
    "cremad_emotions = CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name())\n",
    "eval_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()),cremad_metadata_filename), # ä½¿ç”¨é…ç½®å’Œç”Ÿæˆçš„æ–‡ä»¶å\n",
    "    emotions=cremad_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    max_audio_length=max_audio_len # ä¼ å…¥å‚æ•°\n",
    ")\n",
    "\n",
    "\n",
    "# --- ç¬¬ 5 æ­¥: å®ä¾‹åŒ–ä½ çš„æ•°æ®æ•´ç†å™¨ ---\n",
    "# æ•°æ®æ•´ç†å™¨å¯¹äºè®­ç»ƒé›†å’Œè¯„ä¼°é›†æ˜¯é€šç”¨çš„\n",
    "data_collator = CustomDataCollator(\n",
    "    audio_processor=audio_processor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    # device=device # device æ¥è‡ªä½ çš„CONFIGæˆ–Notebooké¡¶éƒ¨å®šä¹‰\n",
    ")\n",
    "\n",
    "# --- ç¬¬ 6 æ­¥: åˆ›å»ºè®­ç»ƒé›†çš„ DataLoader ---\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=True, # è®­ç»ƒé›†é€šå¸¸éœ€è¦æ‰“ä¹±\n",
    "    collate_fn=data_collator, # å…³é”®ï¼åœ¨è¿™é‡Œä¼ å…¥ä½ çš„è‡ªå®šä¹‰æ•´ç†å™¨\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator, \n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'], \n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "# --- éªŒè¯batch size ---\n",
    "print(train_dataloader.batch_size)\n",
    "\n",
    "# --- ç¬¬ 7 æ­¥: åˆ›å»ºè¯„ä¼°é›†çš„ DataLoader ---\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=False, # è¯„ä¼°é›†é€šå¸¸ä¸éœ€è¦æ‰“ä¹±\n",
    "    collate_fn=data_collator, # å…³é”®ï¼åœ¨è¿™é‡Œä¼ å…¥ä½ çš„è‡ªå®šä¹‰æ•´ç†å™¨\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf763c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- éªŒè¯ DataLoader å’Œ Collator (è¯¦ç»†è°ƒè¯•æ¨¡å¼) ---\n",
      "\n",
      "éªŒè¯è®­ç»ƒé›† DataLoader:\n",
      "æˆåŠŸä» DataLoader è·å– CPU æ‰¹æ¬¡ã€‚\n",
      "\n",
      "å¼€å§‹é€ä¸ªå°†å¼ é‡ç§»åŠ¨åˆ° GPU...\n",
      "å‡†å¤‡ç§»åŠ¨ 'audio_input_values'... | ç±»å‹: torch.float32 | å½¢çŠ¶: torch.Size([1, 32, 160000])\n",
      "'audio_input_values' ç§»åŠ¨æˆåŠŸï¼\n",
      "å‡†å¤‡ç§»åŠ¨ 'text_input_ids'... | ç±»å‹: torch.int64 | å½¢çŠ¶: torch.Size([32, 54])\n",
      "'text_input_ids' ç§»åŠ¨æˆåŠŸï¼\n",
      "å‡†å¤‡ç§»åŠ¨ 'text_attention_mask'... | ç±»å‹: torch.int64 | å½¢çŠ¶: torch.Size([32, 54])\n",
      "'text_attention_mask' ç§»åŠ¨æˆåŠŸï¼\n",
      "å‡†å¤‡ç§»åŠ¨ 'labels'... | ç±»å‹: torch.int64 | å½¢çŠ¶: torch.Size([32])\n",
      "'labels' ç§»åŠ¨æˆåŠŸï¼\n",
      "\n",
      "æ‰€æœ‰å¼ é‡å‡å·²æˆåŠŸç§»åŠ¨åˆ° GPUï¼\n",
      "--- éªŒè¯ DataLoader å’Œ Collator ---\n",
      "\n",
      "éªŒè¯è®­ç»ƒé›† DataLoader:\n",
      "æˆåŠŸä»è®­ç»ƒé›† DataLoader è·å–ä¸€ä¸ªæ‰¹æ¬¡ï¼\n",
      "æ‰¹æ¬¡åŒ…å«çš„é”®: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "éŸ³é¢‘è¾“å…¥å½¢çŠ¶: torch.Size([1, 32, 160000])\n",
      "æ–‡æœ¬è¾“å…¥å½¢çŠ¶: torch.Size([32, 53])\n",
      "æ ‡ç­¾å½¢çŠ¶: torch.Size([32])\n",
      "\n",
      "éªŒè¯è¯„ä¼°é›† DataLoader:\n",
      "æˆåŠŸä»è¯„ä¼°é›† DataLoader è·å–ä¸€ä¸ªæ‰¹æ¬¡ï¼\n",
      "æ‰¹æ¬¡åŒ…å«çš„é”®: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "éŸ³é¢‘è¾“å…¥å½¢çŠ¶: torch.Size([1, 32, 160000])\n",
      "æ–‡æœ¬è¾“å…¥å½¢çŠ¶: torch.Size([32, 11])\n",
      "æ ‡ç­¾å½¢çŠ¶: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# --- ç¬¬ 8 æ­¥ (éªŒè¯): ç²¾ç¡®å®šä½é—®é¢˜çš„è¯¦ç»†è°ƒè¯• ---\n",
    "print(\"--- éªŒè¯ DataLoader å’Œ Collator (è¯¦ç»†è°ƒè¯•æ¨¡å¼) ---\")\n",
    "print(\"\\néªŒè¯è®­ç»ƒé›† DataLoader:\")\n",
    "\n",
    "# åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ gpu_batch å­—å…¸\n",
    "gpu_batch = {}\n",
    "try:\n",
    "    # 1. è·å– CPU æ‰¹æ¬¡ (è¿™ä¸€æ­¥å·²ç»æˆåŠŸ)\n",
    "    cpu_batch = next(iter(train_dataloader))\n",
    "    print(\"æˆåŠŸä» DataLoader è·å– CPU æ‰¹æ¬¡ã€‚\")\n",
    "\n",
    "    # 2. é€ä¸ªæ£€æŸ¥å¹¶ç§»åŠ¨å¼ é‡\n",
    "    print(\"\\nå¼€å§‹é€ä¸ªå°†å¼ é‡ç§»åŠ¨åˆ° GPU...\")\n",
    "\n",
    "    # æ£€æŸ¥ 'audio_input_values'\n",
    "    key = 'audio_input_values'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"å‡†å¤‡ç§»åŠ¨ '{key}'... | ç±»å‹: {tensor.dtype} | å½¢çŠ¶: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' ç§»åŠ¨æˆåŠŸï¼\")\n",
    "\n",
    "    # æ£€æŸ¥ 'text_input_ids'\n",
    "    key = 'text_input_ids'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"å‡†å¤‡ç§»åŠ¨ '{key}'... | ç±»å‹: {tensor.dtype} | å½¢çŠ¶: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' ç§»åŠ¨æˆåŠŸï¼\")\n",
    "\n",
    "    # æ£€æŸ¥ 'text_attention_mask'\n",
    "    key = 'text_attention_mask'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"å‡†å¤‡ç§»åŠ¨ '{key}'... | ç±»å‹: {tensor.dtype} | å½¢çŠ¶: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' ç§»åŠ¨æˆåŠŸï¼\")\n",
    "\n",
    "    # æ£€æŸ¥ 'labels'\n",
    "    key = 'labels'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"å‡†å¤‡ç§»åŠ¨ '{key}'... | ç±»å‹: {tensor.dtype} | å½¢çŠ¶: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' ç§»åŠ¨æˆåŠŸï¼\")\n",
    "\n",
    "    print(\"\\næ‰€æœ‰å¼ é‡å‡å·²æˆåŠŸç§»åŠ¨åˆ° GPUï¼\")\n",
    "\n",
    "except Exception as e:\n",
    "    # å¦‚æœå‡ºé”™ï¼Œæˆ‘ä»¬ä¼šæ˜ç¡®çŸ¥é“æ˜¯åœ¨å¤„ç†å“ªä¸ª key æ—¶å‘ç”Ÿçš„\n",
    "    print(f\"\\nåœ¨å°è¯•ç§»åŠ¨ '{key}' å¼ é‡æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "# --- ç¬¬ 8 æ­¥ (éªŒè¯): ä»DataLoaderä¸­å–å‡ºä¸€ä¸ªæ‰¹æ¬¡ï¼Œæ£€æŸ¥å…¶å†…å®¹ ---\n",
    "print(\"--- éªŒè¯ DataLoader å’Œ Collator ---\")\n",
    "print(\"\\néªŒè¯è®­ç»ƒé›† DataLoader:\")\n",
    "try:\n",
    "    first_train_batch = next(iter(train_dataloader))\n",
    "    print(\"æˆåŠŸä»è®­ç»ƒé›† DataLoader è·å–ä¸€ä¸ªæ‰¹æ¬¡ï¼\")\n",
    "    print(\"æ‰¹æ¬¡åŒ…å«çš„é”®:\", first_train_batch.keys())\n",
    "    print(\"éŸ³é¢‘è¾“å…¥å½¢çŠ¶:\", first_train_batch['audio_input_values'].shape)\n",
    "    print(\"æ–‡æœ¬è¾“å…¥å½¢çŠ¶:\", first_train_batch['text_input_ids'].shape)\n",
    "    print(\"æ ‡ç­¾å½¢çŠ¶:\", first_train_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"è·å–è®­ç»ƒé›†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "print(\"\\néªŒè¯è¯„ä¼°é›† DataLoader:\")\n",
    "try:\n",
    "    first_eval_batch = next(iter(eval_dataloader))\n",
    "    print(\"æˆåŠŸä»è¯„ä¼°é›† DataLoader è·å–ä¸€ä¸ªæ‰¹æ¬¡ï¼\")\n",
    "    print(\"æ‰¹æ¬¡åŒ…å«çš„é”®:\", first_eval_batch.keys())\n",
    "    print(\"éŸ³é¢‘è¾“å…¥å½¢çŠ¶:\", first_eval_batch['audio_input_values'].shape)\n",
    "    print(\"æ–‡æœ¬è¾“å…¥å½¢çŠ¶:\", first_eval_batch['text_input_ids'].shape)\n",
    "    print(\"æ ‡ç­¾å½¢çŠ¶:\", first_eval_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"è·å–è¯„ä¼°é›†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde94d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- åˆå§‹åŒ–åŸºçº¿æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] æ­£åœ¨å†»ç»“WavLMçš„ç‰¹å¾æå–å±‚...\n",
      "[INFO] ç‰¹å¾æå–å±‚å·²å†»ç»“ã€‚\n",
      "\n",
      "--- å¼€å§‹åœ¨ IEMOCAP ä¸Šè®­ç»ƒåŸºçº¿æ¨¡å‹ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|â–ˆâ–ˆâ–ˆâ–      | 38/113 [2:23:33<4:43:21, 226.68s/it, accuracy=0.406, loss=1.3]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --- æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹ ---\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- å¼€å§‹åœ¨ IEMOCAP ä¸Šè®­ç»ƒåŸºçº¿æ¨¡å‹ ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mbaseline_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- æ­¥éª¤ 4: åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼° ---\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\core\\trainer.py:57\u001b[39m, in \u001b[36mAbstractTrainer.train\u001b[39m\u001b[34m(self, train_dataloader)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "# å®ä¾‹åŒ–æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\n",
    "print(\"\\n--- åˆå§‹åŒ–åŸºçº¿æ¨¡å‹å’Œè®­ç»ƒå™¨ ---\")\n",
    "num_labels = len(iemocap_emotions)\n",
    "baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "baseline_trainer = AudioBaselineTrainer(\n",
    "    model=baseline_model,\n",
    "    num_epochs=CONFIG.training_epochs(),\n",
    "    learning_rate=CONFIG.learning_rate(),\n",
    "    optimizer_type=CONFIG.optimizer_type()\n",
    ")\n",
    "\n",
    "# --- æ­¥éª¤ 3: è®­ç»ƒæ¨¡å‹ ---\n",
    "print(\"\\n--- å¼€å§‹åœ¨ IEMOCAP ä¸Šè®­ç»ƒåŸºçº¿æ¨¡å‹ ---\")\n",
    "baseline_trainer.train(train_dataloader)\n",
    "\n",
    "# --- æ­¥éª¤ 4: åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼° ---\n",
    "print(\"\\n--- åœ¨ IEMOCAP éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ ---\")\n",
    "baseline_trainer.eval(val_dataloader, labels=iemocap_emotions)\n",
    "\n",
    "# --- æ­¥éª¤ 5: åœ¨ CREMA-D æµ‹è¯•é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° ---\n",
    "print(\"\\n--- åœ¨ CREMA-D æµ‹è¯•é›†ä¸Šè¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° ---\")\n",
    "baseline_trainer.eval(eval_dataloader, labels=cremad_emotions)\n",
    "\n",
    "print(\"\\n--- åŸºçº¿æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼ ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56fd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # è®­ç»ƒå‰å…ˆæ¸…ç†ä¸€æ¬¡æ˜¾å­˜\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
