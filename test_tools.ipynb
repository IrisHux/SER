{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032a6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad571a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Path: E:/Unitec/SER/audio\\dataset\\IEMOCAP\n",
      "Evaluation Dataset Path: E:/Unitec/SER/audio\\dataset\\CREMA-D\n",
      "IEMOCAP Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\n",
      "CREMA-D Preprocessed Dir: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\n",
      "IEMOCAP Emotions: ['ang', 'neu', 'sad', 'hap']\n",
      "CREMA-D Emotions: ['ang', 'neu', 'sad', 'hap']\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    print(\"Training Dataset Path:\", CONFIG.dataset_path(\"training\"))\n",
    "    print(\"Evaluation Dataset Path:\", CONFIG.dataset_path(\"evaluation\"))\n",
    "    print(\"IEMOCAP Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Preprocessed Dir:\", CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()))\n",
    "    print(\"IEMOCAP Emotions:\", CONFIG.dataset_emotions(CONFIG.training_dataset_name()))\n",
    "    print(\"CREMA-D Emotions:\", CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad4e5e1",
   "metadata": {},
   "source": [
    "## test processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b6db8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F000.wav</td>\n",
       "      <td>Excuse me.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F001.wav</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F002.wav</td>\n",
       "      <td>Is there a problem?</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F005.wav</td>\n",
       "      <td>Well what's the problem?  Let me change it.</td>\n",
       "      <td>neu</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...</td>\n",
       "      <td>Ses01F_impro01_F012.wav</td>\n",
       "      <td>That's out of control.</td>\n",
       "      <td>ang</td>\n",
       "      <td>Session1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path           audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F000.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F001.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F002.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F005.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Session1\\s...  Ses01F_impro01_F012.wav   \n",
       "\n",
       "                                          text emotion   session  \n",
       "0                                   Excuse me.     neu  Session1  \n",
       "1                                        Yeah.     neu  Session1  \n",
       "2                          Is there a problem?     neu  Session1  \n",
       "3  Well what's the problem?  Let me change it.     neu  Session1  \n",
       "4                       That's out of control.     ang  Session1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4490 entries, 0 to 4489\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4490 non-null   object\n",
      " 1   audio_filename  4490 non-null   object\n",
      " 2   text            4490 non-null   object\n",
      " 3   emotion         4490 non-null   object\n",
      " 4   session         4490 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 175.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "neu    1708\n",
       "ang    1103\n",
       "sad    1084\n",
       "hap     595\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per Session:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "session\n",
       "Session3    1000\n",
       "Session1     942\n",
       "Session5     942\n",
       "Session2     813\n",
       "Session4     793\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from core.config import CONFIG\n",
    "from preprocessing.iemocap import IemocapPreprocessor\n",
    "\n",
    "# 确保 CONFIG 已经加载了配置文件\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# 初始化 IemocapPreprocessor，使用配置中训练数据集的路径\n",
    "# 假设您想验证训练数据的加载\n",
    "iemocap_dataset_path = CONFIG.dataset_path(\"training\")\n",
    "iemocap_preprocessor = IemocapPreprocessor(iemocap_dataset_path)\n",
    "\n",
    "# 生成 DataFrame\n",
    "iemocap_df = iemocap_preprocessor.generate_dataframe()\n",
    "\n",
    "# 显示 DataFrame 的前几行\n",
    "print(\"DataFrame head:\")\n",
    "display(iemocap_df.head())\n",
    "\n",
    "# 您还可以打印一些关于 DataFrame 的信息来进一步验证\n",
    "print(\"\\nDataFrame Info:\")\n",
    "iemocap_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(iemocap_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per Session:\")\n",
    "display(iemocap_df['session'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823ad63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_path</th>\n",
       "      <th>audio_filename</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>hap</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>neu</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_DFA_SAD_XX.wav</td>\n",
       "      <td>Don't forget a jacket</td>\n",
       "      <td>sad</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...</td>\n",
       "      <td>1001_IEO_ANG_HI.wav</td>\n",
       "      <td>It's eleven o'clock</td>\n",
       "      <td>ang</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          audio_path       audio_filename  \\\n",
       "0  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_ANG_XX.wav   \n",
       "1  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_HAP_XX.wav   \n",
       "2  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_NEU_XX.wav   \n",
       "3  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_DFA_SAD_XX.wav   \n",
       "4  E:/Unitec/SER/audio\\dataset\\CREMA-D\\AudioWAV\\1...  1001_IEO_ANG_HI.wav   \n",
       "\n",
       "                    text emotion speaker  \n",
       "0  Don't forget a jacket     ang    1001  \n",
       "1  Don't forget a jacket     hap    1001  \n",
       "2  Don't forget a jacket     neu    1001  \n",
       "3  Don't forget a jacket     sad    1001  \n",
       "4    It's eleven o'clock     ang    1001  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4900 entries, 0 to 4899\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   audio_path      4900 non-null   object\n",
      " 1   audio_filename  4900 non-null   object\n",
      " 2   text            4900 non-null   object\n",
      " 3   emotion         4900 non-null   object\n",
      " 4   speaker         4900 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 191.5+ KB\n",
      "\n",
      "Emotion Distribution:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "ang    1271\n",
       "hap    1271\n",
       "sad    1271\n",
       "neu    1087\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data per speaker:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "1001    54\n",
       "1047    54\n",
       "1067    54\n",
       "1066    54\n",
       "1065    54\n",
       "        ..\n",
       "1076    53\n",
       "1002    53\n",
       "1009    50\n",
       "1008    50\n",
       "1019    50\n",
       "Name: count, Length: 91, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from preprocessing.cremad import CremaDPreprocessor\n",
    "\n",
    "# 假设您想验证训练数据的加载\n",
    "cremad_dataset_path = CONFIG.dataset_path(\"evaluation\")\n",
    "cremad_preprocessor = CremaDPreprocessor(cremad_dataset_path)\n",
    "\n",
    "# 生成 DataFrame\n",
    "cremad_df = cremad_preprocessor.generate_dataframe()\n",
    "\n",
    "# 显示 DataFrame 的前几行\n",
    "print(\"DataFrame head:\")\n",
    "display(cremad_df.head())\n",
    "\n",
    "# 您还可以打印一些关于 DataFrame 的信息来进一步验证\n",
    "print(\"\\nDataFrame Info:\")\n",
    "cremad_df.info()\n",
    "\n",
    "print(\"\\nEmotion Distribution:\")\n",
    "display(cremad_df['emotion'].value_counts())\n",
    "\n",
    "print(\"\\nData per speaker:\")\n",
    "display(cremad_df['speaker'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c225ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing models and config ---\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: IEMOCAP\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'iemocap_raw.pkl' ---\n",
      "[INFO] Using IemocapPreprocessor for dataset: IEMOCAP\n",
      "[INFO] Target emotions being extracted: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] Preprocessing complete. Total entries extracted: 4490\n",
      "[INFO] Emotion distribution:\n",
      "emotion\n",
      "neu    1708\n",
      "ang    1103\n",
      "sad    1084\n",
      "hap     595\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per session:\n",
      "session\n",
      "Session3    1000\n",
      "Session1     942\n",
      "Session5     942\n",
      "Session2     813\n",
      "Session4     793\n",
      "Name: count, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\IEMOCAP\\Preprocessed\\iemocap_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for IEMOCAP.\n",
      "\n",
      "====================\n",
      "[START] Processing dataset: CREMA-D\n",
      "====================\n",
      "--- Step 1: Processing raw data to 'crema-d_raw.pkl' ---\n",
      "[INFO] Using CremaDPreprocessor for dataset: CREMA-D\n",
      "[INFO] Target emotions being extracted for CREMA-D: ['ang', 'neu', 'sad', 'hap']\n",
      "\n",
      "[INFO] CREMA-D Preprocessing complete. Total entries extracted: 4900\n",
      "[INFO] Emotion distribution for CREMA-D:\n",
      "emotion\n",
      "ang    1271\n",
      "hap    1271\n",
      "sad    1271\n",
      "neu    1087\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Data per Speaker:\n",
      "speaker\n",
      "1001    54\n",
      "1047    54\n",
      "1067    54\n",
      "1066    54\n",
      "1065    54\n",
      "        ..\n",
      "1076    53\n",
      "1002    53\n",
      "1009    50\n",
      "1008    50\n",
      "1019    50\n",
      "Name: count, Length: 91, dtype: int64\n",
      "[INFO] Raw data DataFrame saved to: E:/Unitec/SER/audio\\dataset\\CREMA-D\\Preprocessed\\crema-d_raw.pkl\n",
      "\n",
      "[SUCCESS] Finished processing for CREMA-D.\n",
      "\n",
      "====================\n",
      "--- All processing complete! ---\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from audio.extractor import WavLMEmotionExtractor\n",
    "from core.config import CONFIG\n",
    "from scripts.preprocess_data import process_raw_data_to_pickle\n",
    "\n",
    "\n",
    "def run_preprocessing_pipeline(dataset_name: str):\n",
    "    \"\"\"\n",
    "    为一个指定的数据集完整地执行数据预处理的三个步骤。\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): 数据集的名称 (从 CONFIG 中获取)。\n",
    "        audio_extractor: 初始化后的音频特征提取器。\n",
    "        text_tokenizer: 初始化后的文本分词器。\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20}\\n[START] Processing dataset: {dataset_name}\\n{'='*20}\")\n",
    "\n",
    "    # 1. 动态生成文件名，避免硬编码\n",
    "    # 例如从 \"IEMOCAP_full_release\" 生成 \"iemocap\" 作为文件名前缀\n",
    "    base_name = dataset_name.split('_')[0].lower() \n",
    "    raw_file = f\"{base_name}_raw.pkl\"\n",
    "    audio_file = f\"{base_name}_audio_features.pkl\"\n",
    "    text_file = f\"{base_name}_text_tokens.pkl\"\n",
    "\n",
    "    # 2. 按顺序执行数据处理流程\n",
    "    print(f\"--- Step 1: Processing raw data to '{raw_file}' ---\")\n",
    "    process_raw_data_to_pickle(dataset_name, raw_file)\n",
    "\n",
    "\n",
    "    print(f\"\\n[SUCCESS] Finished processing for {dataset_name}.\")\n",
    "\n",
    "\n",
    "# --- 主执行脚本 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 集中进行初始化\n",
    "    print(\"--- Initializing models and config ---\")\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "    # 2. 定义需要处理的数据集列表\n",
    "    datasets_to_process = [\n",
    "        CONFIG.training_dataset_name(),\n",
    "        CONFIG.evaluation_dataset_name()\n",
    "    ]\n",
    "\n",
    "    # 3. 循环调用处理流程\n",
    "    for name in datasets_to_process:\n",
    "        run_preprocessing_pipeline(name)\n",
    "\n",
    "    print(f\"\\n{'='*20}\\n--- All processing complete! ---\\n{'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5c8c4",
   "metadata": {},
   "source": [
    "## 实例化数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b09ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Unitec\\SER\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 IEMOCAP 数据集用于训练...\n",
      "--- 正在为数据集 'IEMOCAP' 准备Dataloaders (实时处理模式) ---\n",
      "[INFO] 已从以下路径加载原始数据信息: E:/Unitec/SER/dataset\\IEMOCAP\\Preprocessed\\iemocap_raw.pkl\n",
      "[INFO] 使用的 processor 类型: <class 'transformers.models.wav2vec2.feature_extraction_wav2vec2.Wav2Vec2FeatureExtractor'>\n"
     ]
    }
   ],
   "source": [
    "from core.config import CONFIG\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- 训练和验证流程 ---\n",
    "print(\"加载 IEMOCAP 数据集用于训练...\")\n",
    "# 只需一行代码，即可获取训练和验证所需的所有 dataloader\n",
    "iemocap_loaders = get_dataloaders(CONFIG.training_dataset_name())\n",
    "train_loader = iemocap_loaders['train']\n",
    "validation_loader = iemocap_loaders['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd0b9d",
   "metadata": {},
   "source": [
    "### train baseline model（New）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a66ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在加载 'IEMOCAP' 数据集用于训练和验证 ---\n",
      "--- 正在为数据集 'IEMOCAP' 准备Dataloaders ---\n",
      "[INFO] 使用内存优化模式\n",
      "[INFO] 已从以下路径加载音频特征: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_audio_features.pkl\n",
      "[INFO] 已从以下路径加载文本Tokens: E:/Iris_project/SER\\dataset\\IEMOCAP\\Preprocessed\\iemocap_text_tokens.pkl\n",
      "[INFO] 使用内存优化的数据整理器\n",
      "[INFO] 已清理加载过程中的临时内存\n",
      "\n",
      "--- 正在加载 'CREMA-D' 数据集用于零样本评估 ---\n",
      "--- 正在为数据集 'CREMA-D' 准备Dataloaders ---\n",
      "[INFO] 使用内存优化模式\n",
      "[INFO] 已从以下路径加载音频特征: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_audio_features.pkl\n",
      "[INFO] 已从以下路径加载文本Tokens: E:/Iris_project/SER\\dataset\\CREMA-D\\Preprocessed\\crema-d_text_tokens.pkl\n",
      "[INFO] 使用内存优化的数据整理器\n",
      "[INFO] 已清理加载过程中的临时内存\n",
      "\n",
      "--- 初始化基线模型和训练器 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base and are newly initialized: ['projector.bias', 'projector.weight', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158: UserWarning: expandable_segments not supported on this platform (Triggered internally at ..\\c10\\cuda\\CUDACachingAllocator.cpp:803.)\n",
      "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\wavlm\\modeling_wavlm.py:1434: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 特征提取层已冻结，使用梯度累积步数: 8\n",
      "\n",
      "--- 开始在 IEMOCAP 上训练基线模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Rerun the training cell\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import MemoryOptimizedAudioBaselineTrainer\n",
    "\n",
    "# 设置CUDA内存优化\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "CONFIG.load_config(\"config.yaml\")\n",
    "\n",
    "# --- 训练和验证流程 ---\n",
    "training_dataset_name = CONFIG.training_dataset_name()\n",
    "print(f\"\\n--- 正在加载 '{training_dataset_name}' 数据集用于训练和验证 ---\")\n",
    "\n",
    "# 使用内存优化版本的dataloaders（这是主要的改动）\n",
    "try:\n",
    "    # 使用内存优化版本\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name, use_memory_optimization=True)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "    \n",
    "    # --- 零样本评估流程 (在 CREMA-D 上) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{evaluation_dataset_name}' 数据集用于零样本评估 ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name, use_memory_optimization=True)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    # --- 实例化模型和训练器 ---\n",
    "    print(\"\\n--- 初始化基线模型和训练器 ---\")\n",
    "    \n",
    "    # 清理内存后创建模型\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 获取情感标签\n",
    "    iemocap_emotions = CONFIG.dataset_emotions(training_dataset_name)\n",
    "    num_labels = len(iemocap_emotions)\n",
    "    \n",
    "    # 创建模型（考虑使用更小的batch_size)\n",
    "    baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "    \n",
    "    # 使用内存优化版本的训练器\n",
    "    baseline_trainer = MemoryOptimizedAudioBaselineTrainer(  # 使用新的训练器\n",
    "        model=baseline_model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate() * 4,  # 由于梯度累积，需要调整学习率\n",
    "        optimizer_type=CONFIG.optimizer_type(),\n",
    "        gradient_accumulation_steps=8  # 梯度累积步数，可以根据需要调整\n",
    "    )\n",
    "    \n",
    "    # --- 步骤 3: 训练模型 ---\n",
    "    print(\"\\n--- 开始在 IEMOCAP 上训练基线模型 ---\")\n",
    "    baseline_trainer.train(train_loader)\n",
    "    \n",
    "    # --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\n",
    "    print(\"\\n--- 在 IEMOCAP 验证集上评估模型性能 ---\")\n",
    "    baseline_trainer.eval(validation_loader, labels=iemocap_emotions)\n",
    "    \n",
    "    # --- 步骤 5: 在 CREMA-D 测试集上进行零样本评估 ---\n",
    "    print(\"\\n--- 在 CREMA-D 测试集上进行零样本评估 ---\")\n",
    "    cremad_emotions = CONFIG.dataset_emotions(evaluation_dataset_name)\n",
    "    baseline_trainer.eval(evaluation_loader, labels=cremad_emotions)\n",
    "    \n",
    "    print(\"\\n--- 基线模型训练和评估完成！ ---\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"\\n[ERROR] CUDA内存不足: {e}\")\n",
    "    print(\"建议:\")\n",
    "    print(\"1. 进一步减小batch_size到1\")\n",
    "    print(\"2. 减少音频最大长度\")\n",
    "    print(\"3. 使用更小的模型variant\")\n",
    "    print(\"4. 重启运行时清理内存\")\n",
    "    \n",
    "    # 清理内存\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] 训练过程中出现错误: {e}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37be2ef",
   "metadata": {},
   "source": [
    "### check cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697ad0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Check PyTorch and CUDA ---\n",
      "Python Version: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
      "PyTorch Version: 2.1.2+cu121\n",
      "Is CUDA available: True\n",
      "\n",
      "--- 2. Get GPU Device Information ---\n",
      "Default CUDA device: cuda:0\n",
      "GPU Name: NVIDIA L40S-6Q\n",
      "PyTorch compiled with CUDA version: 12.1\n",
      "\n",
      "--- 3. Test Data Transfer Between CPU and GPU ---\n",
      "a. Tensor created on the CPU: tensor([1, 2, 3])\n",
      "   - Device: cpu\n",
      "\n",
      "[ERROR] Failed to move data to GPU: CUDA error: operation not supported\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"--- 1. Check PyTorch and CUDA ---\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "print(f\"Is CUDA available: {is_cuda_available}\")\n",
    "\n",
    "if not is_cuda_available:\n",
    "    print(\"\\n[ERROR] PyTorch could not detect CUDA. Please check your NVIDIA driver and PyTorch installation.\")\n",
    "    # If CUDA is not available, exit the script\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 2. Get GPU Device Information ---\")\n",
    "# Get the default CUDA device (usually GPU 0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(f\"Default CUDA device: {device}\")\n",
    "\n",
    "# Print the name of the GPU\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"GPU Name: {gpu_name}\")\n",
    "\n",
    "# Print the CUDA version PyTorch was compiled with\n",
    "torch_cuda_version = torch.version.cuda\n",
    "print(f\"PyTorch compiled with CUDA version: {torch_cuda_version}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 3. Test Data Transfer Between CPU and GPU ---\")\n",
    "# a. Create a tensor on the CPU\n",
    "cpu_tensor = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(f\"a. Tensor created on the CPU: {cpu_tensor}\")\n",
    "print(f\"   - Device: {cpu_tensor.device}\")\n",
    "\n",
    "# b. Try to move the tensor to the GPU\n",
    "try:\n",
    "    gpu_tensor = cpu_tensor.to(device)\n",
    "    print(f\"\\nb. Successfully moved tensor to GPU: {gpu_tensor}\")\n",
    "    print(f\"   - Device: {gpu_tensor.device}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move data to GPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- 4. Test Computation on GPU ---\")\n",
    "# a. Create two tensors on the GPU for computation\n",
    "try:\n",
    "    a = torch.randn(3, 3).to(device)\n",
    "    b = torch.randn(3, 3).to(device)\n",
    "    print(f\"a. Created two 3x3 random tensors on the GPU.\")\n",
    "    print(f\"   - Tensor a device: {a.device}\")\n",
    "    print(f\"   - Tensor b device: {b.device}\")\n",
    "\n",
    "    # b. Perform matrix multiplication on the GPU\n",
    "    print(\"\\nb. Performing matrix multiplication on GPU (c = a * b)...\")\n",
    "    c = torch.matmul(a, b)\n",
    "    print(f\"   - Result c device: {c.device}\")\n",
    "    print(f\"   - Computation successful!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Computation on GPU failed: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(\"\\n--- 5. Test Moving Result Back to CPU ---\")\n",
    "# a. Move the computation result from GPU back to CPU\n",
    "try:\n",
    "    result_cpu_tensor = c.cpu()\n",
    "    print(\"a. Successfully moved the computation result back to the CPU.\")\n",
    "    print(f\"   - Device: {result_cpu_tensor.device}\")\n",
    "    print(\"\\nComputation result:\")\n",
    "    print(result_cpu_tensor)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Failed to move result back to CPU: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "print(\"\\n--- All tests completed ---\")\n",
    "print(\"[SUCCESS] Your PyTorch and CUDA environment is configured correctly, and they can communicate and perform computations normally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ce7a1f",
   "metadata": {},
   "source": [
    "### （pass）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa1a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mget_dataloaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataloaders\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbaseline_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineModel\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioBaselineTrainer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      8\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    主函数，用于执行声学基线模型的完整训练和评估流程。\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\audio\\trainer.py:16\u001b[39m\n\u001b[32m     13\u001b[39m torch.set_float32_matmul_precision(\u001b[33m\"\u001b[39m\u001b[33mhigh\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 可选：首次预热，避免第一次调用卡顿\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m _ = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m logger = logging.getLogger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# class AudioTrainer(AbstractTrainer):\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m#     def __init__(\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#         self,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 假设 AbstractTrainer 已经从 core/trainer.py 导入或在notebook中定义\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: operation not supported\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from core.config import CONFIG, device\n",
    "from scripts.get_dataloaders import get_dataloaders\n",
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函数，用于执行声学基线模型的完整训练和评估流程。\n",
    "    \"\"\"\n",
    "    # 1. 加载配置文件\n",
    "    CONFIG.load_config(\"config.yaml\")\n",
    "    print(f\"--- 实验配置已加载 ---\")\n",
    "    print(f\"使用的设备: {device}\")\n",
    "\n",
    "    # --- 训练和验证流程 ---\n",
    "    training_dataset_name = CONFIG.training_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{training_dataset_name}' 数据集用于训练和验证 ---\")\n",
    "    \n",
    "    # 使用高级函数获取训练和验证所需的所有 dataloader\n",
    "    iemocap_loaders = get_dataloaders(training_dataset_name)\n",
    "    train_loader = iemocap_loaders['train']\n",
    "    validation_loader = iemocap_loaders['validation']\n",
    "\n",
    "    # 3. 初始化模型\n",
    "    # 从配置中获取情感标签列表，以确定模型的输出维度\n",
    "    num_labels = len(CONFIG.dataset_emotions(training_dataset_name))\n",
    "    print(f\"\\n--- 正在初始化 AudioBaselineModel (类别数: {num_labels}) ---\")\n",
    "    model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "    # 4. 初始化训练器\n",
    "    print(f\"--- 正在初始化 AudioBaselineTrainer ---\")\n",
    "    trainer = AudioBaselineTrainer(\n",
    "        model=model,\n",
    "        num_epochs=CONFIG.training_epochs(),\n",
    "        learning_rate=CONFIG.learning_rate(),\n",
    "        optimizer_type=CONFIG.optimizer_type()\n",
    "    )\n",
    "\n",
    "    # 5. 开始训练\n",
    "    trainer.train(train_loader)\n",
    "\n",
    "    # 6. 在验证集上评估\n",
    "    print(f\"\\n--- 正在 '{training_dataset_name}' 的验证集上进行评估 ---\")\n",
    "    trainer.eval(validation_loader, labels=CONFIG.dataset_emotions(training_dataset_name))\n",
    "\n",
    "    # --- 零样本评估流程 (在 CREMA-D 上) ---\n",
    "    evaluation_dataset_name = CONFIG.evaluation_dataset_name()\n",
    "    print(f\"\\n--- 正在加载 '{evaluation_dataset_name}' 数据集用于零样本评估 ---\")\n",
    "    \n",
    "    cremad_loaders = get_dataloaders(evaluation_dataset_name)\n",
    "    evaluation_loader = cremad_loaders['evaluation']\n",
    "    \n",
    "    print(f\"\\n--- 正在 '{evaluation_dataset_name}' 上进行零样本评估 ---\")\n",
    "    trainer.eval(evaluation_loader, labels=CONFIG.dataset_emotions(evaluation_dataset_name))\n",
    "    \n",
    "    print(\"\\n--- 训练和评估流程全部完成 ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d06293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "已创建训练集，包含 3592 个样本。\n",
      "已创建验证集，包含 898 个样本。\n",
      "已创建训练集，包含 3920 个样本。\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 第 1 步: 加载必要的库和你的自定义类 ---\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2FeatureExtractor, DebertaV2Tokenizer\n",
    "import os\n",
    "\n",
    "from dataloaders.dataset import CustomSERDataset\n",
    "from scripts.get_dataloaders import CustomDataCollator\n",
    "\n",
    "\n",
    "# --- 确定计算设备 ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 第 2 步: 初始化处理器和分词器 ---\n",
    "# 这些是collator需要的 \"工具\"\n",
    "audio_processor = Wav2Vec2FeatureExtractor.from_pretrained(CONFIG.audio_encoder_name())\n",
    "text_tokenizer = DebertaV2Tokenizer.from_pretrained(CONFIG.text_encoder_name())\n",
    "\n",
    "# --- 第 3 步: 实例化训练数据集 (IEMOCAP) ---\n",
    "# 注意：你需要先运行你的预处理脚本，生成统一的元数据文件\n",
    "\n",
    "\n",
    "# 定义最大长度 (例如10秒)\n",
    "MAX_LEN_IN_SECONDS = 10\n",
    "max_audio_len = 16000 * MAX_LEN_IN_SECONDS\n",
    "\n",
    "iemocap_emotions = CONFIG.dataset_emotions(CONFIG.training_dataset_name())\n",
    "train_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename), # 使用配置和生成的文件名\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='train',\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "# 创建IEMOCAP验证数据集\n",
    "val_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.training_dataset_name()),iemocap_metadata_filename),\n",
    "    emotions=iemocap_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    split='val',\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "# --- 第 4 步: 实例化评估数据集 (CREMA-D) ---\n",
    "cremad_emotions = CONFIG.dataset_emotions(CONFIG.evaluation_dataset_name())\n",
    "eval_dataset = CustomSERDataset(\n",
    "    metadata_file_path=os.path.join(CONFIG.dataset_preprocessed_dir_path(CONFIG.evaluation_dataset_name()),cremad_metadata_filename), # 使用配置和生成的文件名\n",
    "    emotions=cremad_emotions,\n",
    "    target_sample_rate=audio_processor.sampling_rate,\n",
    "    max_audio_length=max_audio_len # 传入参数\n",
    ")\n",
    "\n",
    "\n",
    "# --- 第 5 步: 实例化你的数据整理器 ---\n",
    "# 数据整理器对于训练集和评估集是通用的\n",
    "data_collator = CustomDataCollator(\n",
    "    audio_processor=audio_processor,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    # device=device # device 来自你的CONFIG或Notebook顶部定义\n",
    ")\n",
    "\n",
    "# --- 第 6 步: 创建训练集的 DataLoader ---\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=True, # 训练集通常需要打乱\n",
    "    collate_fn=data_collator, # 关键！在这里传入你的自定义整理器\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'], \n",
    "    shuffle=False, \n",
    "    collate_fn=data_collator, \n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'], \n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "# --- 验证batch size ---\n",
    "print(train_dataloader.batch_size)\n",
    "\n",
    "# --- 第 7 步: 创建评估集的 DataLoader ---\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=CONFIG.dataloader_dict()['batch_size'],\n",
    "    shuffle=False, # 评估集通常不需要打乱\n",
    "    collate_fn=data_collator, # 关键！在这里传入你的自定义整理器\n",
    "    # num_workers=CONFIG.dataloader_dict()['num_workers'],\n",
    "    # pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf763c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 验证 DataLoader 和 Collator (详细调试模式) ---\n",
      "\n",
      "验证训练集 DataLoader:\n",
      "成功从 DataLoader 获取 CPU 批次。\n",
      "\n",
      "开始逐个将张量移动到 GPU...\n",
      "准备移动 'audio_input_values'... | 类型: torch.float32 | 形状: torch.Size([1, 32, 160000])\n",
      "'audio_input_values' 移动成功！\n",
      "准备移动 'text_input_ids'... | 类型: torch.int64 | 形状: torch.Size([32, 54])\n",
      "'text_input_ids' 移动成功！\n",
      "准备移动 'text_attention_mask'... | 类型: torch.int64 | 形状: torch.Size([32, 54])\n",
      "'text_attention_mask' 移动成功！\n",
      "准备移动 'labels'... | 类型: torch.int64 | 形状: torch.Size([32])\n",
      "'labels' 移动成功！\n",
      "\n",
      "所有张量均已成功移动到 GPU！\n",
      "--- 验证 DataLoader 和 Collator ---\n",
      "\n",
      "验证训练集 DataLoader:\n",
      "成功从训练集 DataLoader 获取一个批次！\n",
      "批次包含的键: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "音频输入形状: torch.Size([1, 32, 160000])\n",
      "文本输入形状: torch.Size([32, 53])\n",
      "标签形状: torch.Size([32])\n",
      "\n",
      "验证评估集 DataLoader:\n",
      "成功从评估集 DataLoader 获取一个批次！\n",
      "批次包含的键: dict_keys(['audio_input_values', 'text_input_ids', 'text_attention_mask', 'labels'])\n",
      "音频输入形状: torch.Size([1, 32, 160000])\n",
      "文本输入形状: torch.Size([32, 11])\n",
      "标签形状: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# --- 第 8 步 (验证): 精确定位问题的详细调试 ---\n",
    "print(\"--- 验证 DataLoader 和 Collator (详细调试模式) ---\")\n",
    "print(\"\\n验证训练集 DataLoader:\")\n",
    "\n",
    "# 初始化一个空的 gpu_batch 字典\n",
    "gpu_batch = {}\n",
    "try:\n",
    "    # 1. 获取 CPU 批次 (这一步已经成功)\n",
    "    cpu_batch = next(iter(train_dataloader))\n",
    "    print(\"成功从 DataLoader 获取 CPU 批次。\")\n",
    "\n",
    "    # 2. 逐个检查并移动张量\n",
    "    print(\"\\n开始逐个将张量移动到 GPU...\")\n",
    "\n",
    "    # 检查 'audio_input_values'\n",
    "    key = 'audio_input_values'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'text_input_ids'\n",
    "    key = 'text_input_ids'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'text_attention_mask'\n",
    "    key = 'text_attention_mask'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    # 检查 'labels'\n",
    "    key = 'labels'\n",
    "    tensor = cpu_batch[key]\n",
    "    print(f\"准备移动 '{key}'... | 类型: {tensor.dtype} | 形状: {tensor.shape}\")\n",
    "    gpu_batch[key] = tensor.to(device)\n",
    "    print(f\"'{key}' 移动成功！\")\n",
    "\n",
    "    print(\"\\n所有张量均已成功移动到 GPU！\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 如果出错，我们会明确知道是在处理哪个 key 时发生的\n",
    "    print(f\"\\n在尝试移动 '{key}' 张量时出错: {e}\")\n",
    "\n",
    "# --- 第 8 步 (验证): 从DataLoader中取出一个批次，检查其内容 ---\n",
    "print(\"--- 验证 DataLoader 和 Collator ---\")\n",
    "print(\"\\n验证训练集 DataLoader:\")\n",
    "try:\n",
    "    first_train_batch = next(iter(train_dataloader))\n",
    "    print(\"成功从训练集 DataLoader 获取一个批次！\")\n",
    "    print(\"批次包含的键:\", first_train_batch.keys())\n",
    "    print(\"音频输入形状:\", first_train_batch['audio_input_values'].shape)\n",
    "    print(\"文本输入形状:\", first_train_batch['text_input_ids'].shape)\n",
    "    print(\"标签形状:\", first_train_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"获取训练集批次时出错: {e}\")\n",
    "\n",
    "print(\"\\n验证评估集 DataLoader:\")\n",
    "try:\n",
    "    first_eval_batch = next(iter(eval_dataloader))\n",
    "    print(\"成功从评估集 DataLoader 获取一个批次！\")\n",
    "    print(\"批次包含的键:\", first_eval_batch.keys())\n",
    "    print(\"音频输入形状:\", first_eval_batch['audio_input_values'].shape)\n",
    "    print(\"文本输入形状:\", first_eval_batch['text_input_ids'].shape)\n",
    "    print(\"标签形状:\", first_eval_batch['labels'].shape)\n",
    "except Exception as e:\n",
    "    print(f\"获取评估集批次时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde94d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 初始化基线模型和训练器 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WavLMForSequenceClassification were not initialized from the model checkpoint at microsoft/wavlm-base-plus and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'classifier.weight', 'projector.bias', 'projector.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 正在冻结WavLM的特征提取层...\n",
      "[INFO] 特征提取层已冻结。\n",
      "\n",
      "--- 开始在 IEMOCAP 上训练基线模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  34%|███▎      | 38/113 [2:23:33<4:43:21, 226.68s/it, accuracy=0.406, loss=1.3]  \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# --- 步骤 3: 训练模型 ---\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 开始在 IEMOCAP 上训练基线模型 ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mbaseline_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 在 IEMOCAP 验证集上评估模型性能 ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Iris_project\\SER\\core\\trainer.py:57\u001b[39m, in \u001b[36mAbstractTrainer.train\u001b[39m\u001b[34m(self, train_dataloader)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 94.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 17.57 GiB is allocated by PyTorch, and 968.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from audio.baseline_model import AudioBaselineModel\n",
    "from audio.trainer import AudioBaselineTrainer\n",
    "# 实例化模型和训练器 ---\n",
    "print(\"\\n--- 初始化基线模型和训练器 ---\")\n",
    "num_labels = len(iemocap_emotions)\n",
    "baseline_model = AudioBaselineModel(num_labels=num_labels).to(device)\n",
    "\n",
    "baseline_trainer = AudioBaselineTrainer(\n",
    "    model=baseline_model,\n",
    "    num_epochs=CONFIG.training_epochs(),\n",
    "    learning_rate=CONFIG.learning_rate(),\n",
    "    optimizer_type=CONFIG.optimizer_type()\n",
    ")\n",
    "\n",
    "# --- 步骤 3: 训练模型 ---\n",
    "print(\"\\n--- 开始在 IEMOCAP 上训练基线模型 ---\")\n",
    "baseline_trainer.train(train_dataloader)\n",
    "\n",
    "# --- 步骤 4: 在 IEMOCAP 验证集上评估 ---\n",
    "print(\"\\n--- 在 IEMOCAP 验证集上评估模型性能 ---\")\n",
    "baseline_trainer.eval(val_dataloader, labels=iemocap_emotions)\n",
    "\n",
    "# --- 步骤 5: 在 CREMA-D 测试集上进行零样本评估 ---\n",
    "print(\"\\n--- 在 CREMA-D 测试集上进行零样本评估 ---\")\n",
    "baseline_trainer.eval(eval_dataloader, labels=cremad_emotions)\n",
    "\n",
    "print(\"\\n--- 基线模型训练和评估完成！ ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56fd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # 训练前先清理一次显存\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
